{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "      \n",
    "    #RL的参数\n",
    "    parser.add_argument('--bs', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.003, help=\"lr decay\")\n",
    "    parser.add_argument('--tau', type=float, default=0.001)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "\n",
    "    #训练参数\n",
    "    parser.add_argument('--max_episode', type=int, default=5000)\n",
    "    parser.add_argument('--max_step', type=int, default=3600)\n",
    "    parser.add_argument('--max_buffer', type=int, default=10000)\n",
    "    parser.add_argument('--max_total_reward', type=float)\n",
    "    parser.add_argument('--epsilon', type=float, default=0.95)\n",
    "    parser.add_argument('--learning_start', type=int, default=600)\n",
    "    parser.add_argument('--update_freq', type=int, default=5)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import count\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from collections import deque\n",
    "import cityflow\n",
    "import json\n",
    "\n",
    "args = args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "\n",
    "    def sample(self, count):\n",
    "        \"\"\"\n",
    "        samples a random batch from the replay memory buffer\n",
    "        :param count: batch size\n",
    "        :return: batch (numpy array)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "#         print(batch)\n",
    "        s_arr = np.float32([arr[0] for arr in batch])\n",
    "        a_arr = np.float32([arr[1] for arr in batch])\n",
    "        r_arr = np.float32([arr[2] for arr in batch])\n",
    "        s1_arr = np.float32([arr[3] for arr in batch])\n",
    "\n",
    "        return s_arr, a_arr, r_arr, s1_arr\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def add(self, s, a, r, s1):\n",
    "        \"\"\"\n",
    "        adds a particular transaction in the memory buffer\n",
    "        :param s: current state\n",
    "        :param a: action taken\n",
    "        :param r: reward received\n",
    "        :param s1: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        transition = (s,a,r,s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class CityFlowEnv():\n",
    "    '''\n",
    "    Simulator Environment with CityFlow\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        self.env = cityflow.Engine(config_file='examples/config_control.json', thread_num=1)\n",
    "#         self.eng.load_roadnet(config['roadnet'])\n",
    "#         self.eng.load_flow(config['flow'])\n",
    "#         self.config = config\n",
    "        self.num_step = args.max_step\n",
    "        self.lane_phase_info = self.parse_roadnet('examples/roadnet.json') # \"intersection_1_1\"\n",
    "\n",
    "        self.intersection_id = list(self.lane_phase_info.keys())[0]\n",
    "        self.start_lane = self.lane_phase_info[self.intersection_id]['start_lane']\n",
    "        self.phase_list = self.lane_phase_info[self.intersection_id][\"phase\"]\n",
    "        self.phase_startLane_mapping = self.lane_phase_info[self.intersection_id][\"phase_startLane_mapping\"]\n",
    "\n",
    "        self.current_phase = self.phase_list[0]\n",
    "        self.current_phase_time = 0\n",
    "        self.yellow_time = 5\n",
    "\n",
    "        self.phase_log = []\n",
    "\n",
    "    def parse_roadnet(self, roadnetFile):\n",
    "        roadnet = json.load(open(roadnetFile))\n",
    "        lane_phase_info_dict ={}\n",
    "\n",
    "        # many intersections exist in the roadnet and virtual intersection is controlled by signal\n",
    "        for intersection in roadnet[\"intersections\"]:\n",
    "            if intersection['virtual']:\n",
    "                continue\n",
    "            lane_phase_info_dict[intersection['id']] = {\"start_lane\": [],\n",
    "                                                         \"end_lane\": [],\n",
    "                                                         \"phase\": [],\n",
    "                                                         \"phase_startLane_mapping\": {},\n",
    "                                                         \"phase_roadLink_mapping\": {}}\n",
    "            road_links = intersection[\"roadLinks\"]\n",
    "\n",
    "            start_lane = []\n",
    "            end_lane = []\n",
    "            roadLink_lane_pair = {ri: [] for ri in\n",
    "                                  range(len(road_links))}  # roadLink includes some lane_pair: (start_lane, end_lane)\n",
    "\n",
    "            for ri in range(len(road_links)):\n",
    "                road_link = road_links[ri]\n",
    "                for lane_link in road_link[\"laneLinks\"]:\n",
    "                    sl = road_link['startRoad'] + \"_\" + str(lane_link[\"startLaneIndex\"])\n",
    "                    el = road_link['endRoad'] + \"_\" + str(lane_link[\"endLaneIndex\"])\n",
    "                    start_lane.append(sl)\n",
    "                    end_lane.append(el)\n",
    "                    roadLink_lane_pair[ri].append((sl, el))\n",
    "\n",
    "            lane_phase_info_dict[intersection['id']][\"start_lane\"] = sorted(list(set(start_lane)))\n",
    "            lane_phase_info_dict[intersection['id']][\"end_lane\"] = sorted(list(set(end_lane)))\n",
    "\n",
    "            for phase_i in range(1, len(intersection[\"trafficLight\"][\"lightphases\"])):\n",
    "                p = intersection[\"trafficLight\"][\"lightphases\"][phase_i]\n",
    "                lane_pair = []\n",
    "                start_lane = []\n",
    "                for ri in p[\"availableRoadLinks\"]:\n",
    "                    lane_pair.extend(roadLink_lane_pair[ri])\n",
    "                    if roadLink_lane_pair[ri][0][0] not in start_lane:\n",
    "                        start_lane.append(roadLink_lane_pair[ri][0][0])\n",
    "                lane_phase_info_dict[intersection['id']][\"phase\"].append(phase_i)\n",
    "                lane_phase_info_dict[intersection['id']][\"phase_startLane_mapping\"][phase_i] = start_lane\n",
    "                lane_phase_info_dict[intersection['id']][\"phase_roadLink_mapping\"][phase_i] = lane_pair\n",
    "\n",
    "        return lane_phase_info_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.phase_log = []\n",
    "\n",
    "    def step(self, next_phase):\n",
    "        if self.current_phase == next_phase:\n",
    "            self.current_phase_time += 1\n",
    "        else:\n",
    "            self.current_phase = next_phase\n",
    "            self.current_phase_time = 1\n",
    "\n",
    "        self.env.set_tl_phase(self.intersection_id, self.current_phase)\n",
    "        self.env.next_step()\n",
    "        self.phase_log.append(self.current_phase)\n",
    "\n",
    "    def get_state(self):\n",
    "        state = {}\n",
    "        state['lane_vehicle_count'] = self.env.get_lane_vehicle_count()  # {lane_id: lane_count, ...}\n",
    "        state['start_lane_vehicle_count'] = {lane: self.env.get_lane_vehicle_count()[lane] for lane in self.start_lane}\n",
    "        state['lane_waiting_vehicle_count'] = self.env.get_lane_waiting_vehicle_count()  # {lane_id: lane_waiting_count, ...}\n",
    "        state['lane_vehicles'] = self.env.get_lane_vehicles()  # {lane_id: [vehicle1_id, vehicle2_id, ...], ...}\n",
    "        state['vehicle_speed'] = self.env.get_vehicle_speed()  # {vehicle_id: vehicle_speed, ...}\n",
    "        state['vehicle_distance'] = self.env.get_vehicle_distance() # {vehicle_id: distance, ...}\n",
    "        state['current_time'] = self.env.get_current_time()\n",
    "        state['current_phase'] = self.current_phase\n",
    "        state['current_phase_time'] = self.current_phase_time\n",
    "\n",
    "        return state\n",
    "\n",
    "    def get_reward(self):\n",
    "        # a sample reward function which calculates the total of waiting vehicles\n",
    "        lane_waiting_vehicle_count = self.env.get_lane_waiting_vehicle_count()\n",
    "        reward = -1 * sum(list(lane_waiting_vehicle_count.values()))\n",
    "        return reward\n",
    "\n",
    "    def log(self):\n",
    "        #self.eng.print_log(self.config['replay_data_path'] + \"/replay_roadnet.json\",\n",
    "        #                   self.config['replay_data_path'] + \"/replay_flow.json\")\n",
    "        df = pd.DataFrame({self.intersection_id: self.phase_log[:self.num_step]})\n",
    "        if not os.path.exists(self.config['data']):\n",
    "            os.makedirs(self.config[\"data\"])\n",
    "        df.to_csv(os.path.join(self.config['data'], 'signal_plan_template.txt'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, args):\n",
    "        super(Actor, self).__init__()\n",
    "        self.args = args\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc3.weight.data = fanin_init(self.fc3.weight.data.size())\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, action_dim)\n",
    "        self.fc4.weight.data.uniform_(-self.args.lr_decay, self.args.lr_decay)\n",
    "        \n",
    "    def forward(self, state):\n",
    "#         state = state.squeeze(dim=1)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.tanh(self.fc4(x))\n",
    "#         print(x.shape)\n",
    "        x = x.detach().numpy()\n",
    "        \n",
    "        action = []\n",
    "        for i in range(len(x)):\n",
    "            action.append(np.argmax(x[i]))\n",
    "        return action\n",
    "\n",
    "    def forward_train(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.tanh(self.fc4(x))\n",
    "#         print(x.shape)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, args):\n",
    "        super(Critic, self).__init__()\n",
    "        self.args = args\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.fcs1 = nn.Linear(state_dim,256)\n",
    "        self.fcs1.weight.data = fanin_init(self.fcs1.weight.data.size())\n",
    "        self.fcs2 = nn.Linear(256,128)\n",
    "        self.fcs2.weight.data = fanin_init(self.fcs2.weight.data.size())\n",
    "\n",
    "        self.fca1 = nn.Linear(action_dim,128)\n",
    "        self.fca1.weight.data = fanin_init(self.fca1.weight.data.size())\n",
    "\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "\n",
    "        self.fc3 = nn.Linear(128,1)\n",
    "        self.fc3.weight.data.uniform_(-self.args.lr_decay, self.args.lr_decay)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "#         state = state.squeeze(dim=1)\n",
    "#         action = action.squeeze(dim=1)\n",
    "        s1 = F.relu(self.fcs1(state))\n",
    "        s2 = F.relu(self.fcs2(s1))\n",
    "        a1 = F.relu(self.fca1(action))\n",
    "        x = torch.cat((s2,a1),dim=1)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.fc3(x)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "\n",
    "    def __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.X)\n",
    "        dx = dx + self.sigma * np.random.randn(len(self.X))\n",
    "        self.X = self.X + dx\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, replay_buffer, lane_phase_info, args):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.iter = 0\n",
    "        self.loss_critic_save = []\n",
    "        self.loss_actor_save = []\n",
    "        self.args = args\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.args)\n",
    "        self.target_actor = Actor(self.state_dim, self.action_dim, self.args)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), self.args.lr)\n",
    "\n",
    "        self.critic = Critic(self.state_dim, \n",
    "                             self.action_dim, self.args)\n",
    "        self.target_critic = Critic(self.state_dim, \n",
    "                                    self.action_dim, self.args)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), self.args.lr)\n",
    "\n",
    "        self.hard_update(self.target_actor, self.actor)\n",
    "        self.hard_update(self.target_critic, self.critic)\n",
    "        \n",
    "        intersection_id = list(lane_phase_info.keys())[0]\n",
    "        self.phase_list = lane_phase_info[intersection_id]['phase']\n",
    "\n",
    "    def soft_update(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - tau) + param.data * tau\n",
    "            )\n",
    "            \n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "#     def get_exploitation_action(self, state):\n",
    "#         state = Variable(torch.from_numpy(state))\n",
    "#         action = self.target_actor.forward(state).detach()\n",
    "#         return action.data.numpy()\n",
    "\n",
    "    \n",
    "#     def get_exploration_action(self, state):\n",
    "#         state = Variable(torch.from_numpy(state))\n",
    "#         action = self.actor.forward(state).detach()\n",
    "#         new_action = action.data.numpy() + (self.noise.sample())\n",
    "#         return new_action\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() > self.args.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        act_values = self.actor.forward(state)\n",
    "        return act_values[0]  # returns action\n",
    "\n",
    "    \n",
    "    def optimize(self):\n",
    "        s1,a1,r1,s2 = self.replay_buffer.sample(self.args.bs)\n",
    "        s1 = Variable(torch.from_numpy(s1))\n",
    "        a1 = Variable(torch.from_numpy(a1))\n",
    "        r1 = Variable(torch.from_numpy(r1))\n",
    "        s2 = Variable(torch.from_numpy(s2))\n",
    "        s1 = s1.squeeze(dim=1)\n",
    "        s2 = s2.squeeze(dim=1)\n",
    "#         print(s1.shape)\n",
    "#         print(a1.shape)\n",
    "#         print(r1.shape)\n",
    "#         r1 = r1.to(self.args.device)\n",
    "#         r1 = Variable(r1)\n",
    "#         a1 = Variable(a1)\n",
    "#         for i in range(len(s1)):\n",
    "#             s1[i] = Variable(s1[i])\n",
    "#             s2[i] = Variable(s2[i])\n",
    "\n",
    "        # ---------------------- optimize critic ----------------------\n",
    "        # Use target actor exploitation policy here for loss evaluation\n",
    "        # 这里应该是TD的方法\n",
    "        a2 = self.target_actor.forward_train(s2)\n",
    "#         print(a2)\n",
    "        next_val = torch.squeeze(self.target_critic.forward(s2, a2).detach())\n",
    "        y_expected = r1 + self.args.gamma * next_val\n",
    "        y_predicted = torch.squeeze(self.critic.forward(s1, self.actor.forward_train(s1)))\n",
    "        \n",
    "        loss_critic = F.smooth_l1_loss(y_predicted, y_expected)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        self.loss_critic_save.append(loss_critic)\n",
    "        # ---------------------- optimize actor ----------------------\n",
    "        pred_a1 = self.actor.forward_train(s1)\n",
    "        loss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.loss_actor_save.append(loss_actor)\n",
    "\n",
    "        self.soft_update(self.target_actor, self.actor, self.args.tau)\n",
    "        self.soft_update(self.target_critic, self.critic, self.args.tau)\n",
    "\n",
    "    def save_models(self, episode_count):\n",
    "        torch.save(self.target_actor.state_dict(), './Models/' + str(episode_count) + '_actor.pt')\n",
    "        torch.save(self.target_critic.state_dict(), './Models/' + str(episode_count) + '_critic.pt')\n",
    "        \n",
    "    def load_models(self, episode):\n",
    "        self.actor.load_state_dict(torch.load('./Models/' + str(episode) + '_actor.pt'))\n",
    "        self.critic.load_state_dict(torch.load('./Models/' + str(episode) + '_critic.pt'))\n",
    "        utils.hard_update(self.target_actor, self.actor)\n",
    "        utils.hard_update(self.target_critic, self.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityFlowEnv(args)\n",
    "\n",
    "lane_phase_info = env.lane_phase_info\n",
    "intersection_id = list(lane_phase_info.keys())[0]\n",
    "phase_list = lane_phase_info[intersection_id]['phase']\n",
    "\n",
    "s_dim = len(lane_phase_info[intersection_id]['start_lane']) + 1\n",
    "a_dim = len(phase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryBuffer(args.max_buffer)\n",
    "trainer = Trainer(s_dim, a_dim, replay_buffer, lane_phase_info, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, time: 39, acton: 6, reward: -22\n",
      "episode: 0/5000, time: 59, acton: 6, reward: -47\n",
      "episode: 0/5000, time: 79, acton: 6, reward: -71\n",
      "episode: 0/5000, time: 99, acton: 2, reward: -75\n",
      "episode: 0/5000, time: 119, acton: 6, reward: -78\n",
      "episode: 0/5000, time: 139, acton: 6, reward: -105\n",
      "episode: 0/5000, time: 179, acton: 6, reward: -128\n",
      "episode: 0/5000, time: 199, acton: 6, reward: -155\n",
      "episode: 10/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 10/5000, time: 59, acton: 1, reward: -61\n",
      "episode: 10/5000, time: 79, acton: 1, reward: -97\n",
      "episode: 10/5000, time: 139, acton: 1, reward: -148\n",
      "episode: 10/5000, time: 159, acton: 1, reward: -170\n",
      "episode: 20/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 20/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 20/5000, time: 79, acton: 1, reward: -82\n",
      "episode: 20/5000, time: 99, acton: 1, reward: -116\n",
      "episode: 20/5000, time: 119, acton: 1, reward: -149\n",
      "episode: 20/5000, time: 139, acton: 1, reward: -183\n",
      "episode: 20/5000, time: 159, acton: 1, reward: -216\n",
      "episode: 30/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 30/5000, time: 79, acton: 1, reward: -79\n",
      "episode: 30/5000, time: 99, acton: 1, reward: -111\n",
      "episode: 30/5000, time: 119, acton: 1, reward: -144\n",
      "episode: 30/5000, time: 139, acton: 1, reward: -125\n",
      "episode: 30/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 30/5000, time: 199, acton: 1, reward: -222\n",
      "episode: 40/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 40/5000, time: 119, acton: 1, reward: -102\n",
      "episode: 40/5000, time: 179, acton: 1, reward: -182\n",
      "episode: 50/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 50/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 50/5000, time: 79, acton: 1, reward: -86\n",
      "episode: 50/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 50/5000, time: 139, acton: 1, reward: -158\n",
      "episode: 50/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 60/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 60/5000, time: 99, acton: 1, reward: -90\n",
      "episode: 60/5000, time: 139, acton: 1, reward: -138\n",
      "episode: 60/5000, time: 159, acton: 1, reward: -147\n",
      "episode: 60/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 60/5000, time: 199, acton: 1, reward: -221\n",
      "episode: 70/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 70/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 70/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 70/5000, time: 119, acton: 1, reward: -125\n",
      "episode: 70/5000, time: 139, acton: 1, reward: -159\n",
      "episode: 70/5000, time: 159, acton: 1, reward: -193\n",
      "episode: 70/5000, time: 179, acton: 1, reward: -229\n",
      "episode: 80/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 80/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 80/5000, time: 59, acton: 1, reward: -67\n",
      "episode: 80/5000, time: 79, acton: 1, reward: -78\n",
      "episode: 80/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 80/5000, time: 179, acton: 1, reward: -193\n",
      "episode: 90/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 90/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 90/5000, time: 59, acton: 1, reward: -57\n",
      "episode: 90/5000, time: 79, acton: 1, reward: -92\n",
      "episode: 90/5000, time: 99, acton: 1, reward: -95\n",
      "episode: 90/5000, time: 159, acton: 1, reward: -162\n",
      "episode: 90/5000, time: 199, acton: 1, reward: -220\n",
      "episode: 100/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 100/5000, time: 59, acton: 1, reward: -41\n",
      "episode: 100/5000, time: 79, acton: 1, reward: -72\n",
      "episode: 100/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 100/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 100/5000, time: 179, acton: 1, reward: -200\n",
      "episode: 100/5000, time: 199, acton: 1, reward: -212\n",
      "episode: 110/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 110/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 110/5000, time: 59, acton: 1, reward: -44\n",
      "episode: 110/5000, time: 99, acton: 1, reward: -84\n",
      "episode: 110/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 110/5000, time: 139, acton: 1, reward: -135\n",
      "episode: 110/5000, time: 159, acton: 1, reward: -168\n",
      "episode: 110/5000, time: 179, acton: 1, reward: -182\n",
      "episode: 110/5000, time: 199, acton: 1, reward: -196\n",
      "episode: 120/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 120/5000, time: 59, acton: 1, reward: -59\n",
      "episode: 120/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 120/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 120/5000, time: 199, acton: 1, reward: -213\n",
      "episode: 130/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 130/5000, time: 79, acton: 1, reward: -83\n",
      "episode: 130/5000, time: 99, acton: 1, reward: -90\n",
      "episode: 130/5000, time: 119, acton: 1, reward: -129\n",
      "episode: 130/5000, time: 179, acton: 1, reward: -195\n",
      "episode: 140/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 140/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 140/5000, time: 99, acton: 4, reward: -74\n",
      "episode: 140/5000, time: 119, acton: 1, reward: -88\n",
      "episode: 140/5000, time: 139, acton: 1, reward: -125\n",
      "episode: 140/5000, time: 159, acton: 1, reward: -142\n",
      "episode: 140/5000, time: 179, acton: 1, reward: -178\n",
      "episode: 140/5000, time: 199, acton: 1, reward: -209\n",
      "episode: 150/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 150/5000, time: 59, acton: 1, reward: -47\n",
      "episode: 150/5000, time: 79, acton: 1, reward: -67\n",
      "episode: 150/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 150/5000, time: 119, acton: 1, reward: -133\n",
      "episode: 150/5000, time: 139, acton: 1, reward: -167\n",
      "episode: 150/5000, time: 159, acton: 1, reward: -165\n",
      "episode: 150/5000, time: 179, acton: 1, reward: -204\n",
      "episode: 150/5000, time: 199, acton: 1, reward: -237\n",
      "episode: 160/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 160/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 160/5000, time: 59, acton: 1, reward: -66\n",
      "episode: 160/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 160/5000, time: 139, acton: 1, reward: -159\n",
      "episode: 160/5000, time: 159, acton: 1, reward: -193\n",
      "episode: 160/5000, time: 179, acton: 1, reward: -225\n",
      "episode: 160/5000, time: 199, acton: 1, reward: -259\n",
      "episode: 170/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 170/5000, time: 79, acton: 1, reward: -65\n",
      "episode: 170/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 170/5000, time: 119, acton: 1, reward: -113\n",
      "episode: 170/5000, time: 139, acton: 1, reward: -147\n",
      "episode: 170/5000, time: 199, acton: 1, reward: -210\n",
      "episode: 180/5000, time: 19, acton: 2, reward: 0\n",
      "episode: 180/5000, time: 59, acton: 1, reward: -47\n",
      "episode: 180/5000, time: 119, acton: 1, reward: -120\n",
      "episode: 180/5000, time: 159, acton: 1, reward: -172\n",
      "episode: 180/5000, time: 179, acton: 1, reward: -207\n",
      "episode: 190/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 190/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 190/5000, time: 99, acton: 1, reward: -94\n",
      "episode: 190/5000, time: 119, acton: 1, reward: -119\n",
      "episode: 190/5000, time: 139, acton: 1, reward: -151\n",
      "episode: 190/5000, time: 159, acton: 1, reward: -171\n",
      "episode: 190/5000, time: 199, acton: 1, reward: -221\n",
      "episode: 200/5000, time: 59, acton: 1, reward: -46\n",
      "episode: 200/5000, time: 79, acton: 1, reward: -80\n",
      "episode: 200/5000, time: 99, acton: 5, reward: -97\n",
      "episode: 200/5000, time: 119, acton: 1, reward: -125\n",
      "episode: 200/5000, time: 159, acton: 1, reward: -171\n",
      "episode: 200/5000, time: 179, acton: 1, reward: -194\n",
      "episode: 200/5000, time: 199, acton: 1, reward: -198\n",
      "episode: 210/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 210/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 210/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 210/5000, time: 79, acton: 1, reward: -101\n",
      "episode: 210/5000, time: 119, acton: 1, reward: -108\n",
      "episode: 210/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 210/5000, time: 159, acton: 1, reward: -181\n",
      "episode: 210/5000, time: 179, acton: 1, reward: -213\n",
      "episode: 210/5000, time: 199, acton: 1, reward: -218\n",
      "episode: 220/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 220/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 220/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 220/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 220/5000, time: 139, acton: 1, reward: -159\n",
      "episode: 220/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 230/5000, time: 79, acton: 1, reward: -76\n",
      "episode: 230/5000, time: 99, acton: 1, reward: -89\n",
      "episode: 230/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 230/5000, time: 139, acton: 1, reward: -158\n",
      "episode: 230/5000, time: 159, acton: 1, reward: -191\n",
      "episode: 230/5000, time: 179, acton: 1, reward: -226\n",
      "episode: 240/5000, time: 19, acton: 1, reward: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 240/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 240/5000, time: 59, acton: 1, reward: -67\n",
      "episode: 240/5000, time: 79, acton: 1, reward: -68\n",
      "episode: 240/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 240/5000, time: 119, acton: 1, reward: -105\n",
      "episode: 240/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 240/5000, time: 199, acton: 1, reward: -209\n",
      "episode: 250/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 250/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 250/5000, time: 59, acton: 1, reward: -50\n",
      "episode: 250/5000, time: 99, acton: 1, reward: -97\n",
      "episode: 250/5000, time: 119, acton: 1, reward: -130\n",
      "episode: 250/5000, time: 139, acton: 1, reward: -140\n",
      "episode: 250/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 250/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 250/5000, time: 199, acton: 1, reward: -239\n",
      "episode: 260/5000, time: 19, acton: 6, reward: 0\n",
      "episode: 260/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 260/5000, time: 59, acton: 1, reward: -63\n",
      "episode: 260/5000, time: 79, acton: 1, reward: -60\n",
      "episode: 260/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 260/5000, time: 119, acton: 1, reward: -134\n",
      "episode: 260/5000, time: 139, acton: 1, reward: -167\n",
      "episode: 260/5000, time: 199, acton: 1, reward: -208\n",
      "episode: 270/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 270/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 270/5000, time: 99, acton: 1, reward: -80\n",
      "episode: 270/5000, time: 139, acton: 1, reward: -135\n",
      "episode: 270/5000, time: 199, acton: 1, reward: -191\n",
      "episode: 280/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 280/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 280/5000, time: 59, acton: 3, reward: -52\n",
      "episode: 280/5000, time: 79, acton: 1, reward: -80\n",
      "episode: 280/5000, time: 99, acton: 1, reward: -114\n",
      "episode: 280/5000, time: 159, acton: 1, reward: -160\n",
      "episode: 280/5000, time: 179, acton: 1, reward: -192\n",
      "episode: 290/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 290/5000, time: 39, acton: 1, reward: -18\n",
      "episode: 290/5000, time: 79, acton: 1, reward: -73\n",
      "episode: 290/5000, time: 119, acton: 1, reward: -120\n",
      "episode: 290/5000, time: 159, acton: 1, reward: -172\n",
      "episode: 290/5000, time: 199, acton: 1, reward: -226\n",
      "episode: 300/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 300/5000, time: 39, acton: 3, reward: -24\n",
      "episode: 300/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 300/5000, time: 79, acton: 1, reward: -89\n",
      "episode: 300/5000, time: 99, acton: 1, reward: -88\n",
      "episode: 300/5000, time: 119, acton: 1, reward: -130\n",
      "episode: 300/5000, time: 139, acton: 1, reward: -162\n",
      "episode: 300/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 300/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 310/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 310/5000, time: 159, acton: 1, reward: -156\n",
      "episode: 310/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 310/5000, time: 199, acton: 1, reward: -195\n",
      "episode: 320/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 320/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 320/5000, time: 99, acton: 1, reward: -102\n",
      "episode: 320/5000, time: 119, acton: 1, reward: -117\n",
      "episode: 320/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 320/5000, time: 179, acton: 1, reward: -193\n",
      "episode: 320/5000, time: 199, acton: 1, reward: -233\n",
      "episode: 330/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 330/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 330/5000, time: 79, acton: 1, reward: -78\n",
      "episode: 330/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 330/5000, time: 139, acton: 1, reward: -150\n",
      "episode: 330/5000, time: 159, acton: 1, reward: -183\n",
      "episode: 330/5000, time: 179, acton: 1, reward: -216\n",
      "episode: 330/5000, time: 199, acton: 1, reward: -251\n",
      "episode: 340/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 340/5000, time: 79, acton: 1, reward: -61\n",
      "episode: 340/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 340/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 340/5000, time: 199, acton: 1, reward: -201\n",
      "episode: 350/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 350/5000, time: 39, acton: 1, reward: -18\n",
      "episode: 350/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 350/5000, time: 79, acton: 1, reward: -70\n",
      "episode: 350/5000, time: 99, acton: 1, reward: -104\n",
      "episode: 350/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 350/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 350/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 350/5000, time: 199, acton: 1, reward: -214\n",
      "episode: 360/5000, time: 59, acton: 1, reward: -50\n",
      "episode: 360/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 360/5000, time: 119, acton: 1, reward: -123\n",
      "episode: 360/5000, time: 139, acton: 1, reward: -133\n",
      "episode: 360/5000, time: 179, acton: 1, reward: -185\n",
      "episode: 360/5000, time: 199, acton: 1, reward: -224\n",
      "episode: 370/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 370/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 370/5000, time: 59, acton: 1, reward: -64\n",
      "episode: 370/5000, time: 79, acton: 1, reward: -101\n",
      "episode: 370/5000, time: 99, acton: 1, reward: -109\n",
      "episode: 370/5000, time: 159, acton: 1, reward: -176\n",
      "episode: 370/5000, time: 179, acton: 1, reward: -210\n",
      "episode: 370/5000, time: 199, acton: 6, reward: -222\n",
      "episode: 380/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 380/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 380/5000, time: 179, acton: 1, reward: -198\n",
      "episode: 380/5000, time: 199, acton: 1, reward: -233\n",
      "episode: 390/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 390/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 390/5000, time: 59, acton: 1, reward: -47\n",
      "episode: 390/5000, time: 79, acton: 1, reward: -73\n",
      "episode: 390/5000, time: 99, acton: 4, reward: -93\n",
      "episode: 390/5000, time: 119, acton: 1, reward: -105\n",
      "episode: 390/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 390/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 400/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 400/5000, time: 59, acton: 1, reward: -64\n",
      "episode: 400/5000, time: 79, acton: 1, reward: -99\n",
      "episode: 400/5000, time: 99, acton: 1, reward: -105\n",
      "episode: 400/5000, time: 119, acton: 1, reward: -114\n",
      "episode: 400/5000, time: 139, acton: 5, reward: -146\n",
      "episode: 400/5000, time: 179, acton: 1, reward: -191\n",
      "episode: 400/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 410/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 410/5000, time: 79, acton: 1, reward: -84\n",
      "episode: 410/5000, time: 99, acton: 1, reward: -119\n",
      "episode: 410/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 420/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 420/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 420/5000, time: 59, acton: 1, reward: -56\n",
      "episode: 420/5000, time: 79, acton: 1, reward: -94\n",
      "episode: 420/5000, time: 99, acton: 1, reward: -126\n",
      "episode: 420/5000, time: 119, acton: 1, reward: -159\n",
      "episode: 420/5000, time: 159, acton: 1, reward: -179\n",
      "episode: 420/5000, time: 179, acton: 1, reward: -190\n",
      "episode: 420/5000, time: 199, acton: 1, reward: -224\n",
      "episode: 430/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 430/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 430/5000, time: 139, acton: 1, reward: -137\n",
      "episode: 430/5000, time: 159, acton: 1, reward: -144\n",
      "episode: 440/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 440/5000, time: 79, acton: 1, reward: -71\n",
      "episode: 440/5000, time: 99, acton: 1, reward: -105\n",
      "episode: 440/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 440/5000, time: 139, acton: 1, reward: -148\n",
      "episode: 440/5000, time: 159, acton: 1, reward: -182\n",
      "episode: 440/5000, time: 199, acton: 1, reward: -226\n",
      "episode: 450/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 450/5000, time: 79, acton: 1, reward: -74\n",
      "episode: 450/5000, time: 99, acton: 1, reward: -82\n",
      "episode: 450/5000, time: 119, acton: 4, reward: -108\n",
      "episode: 450/5000, time: 179, acton: 1, reward: -175\n",
      "episode: 450/5000, time: 199, acton: 1, reward: -201\n",
      "episode: 460/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 460/5000, time: 59, acton: 1, reward: -55\n",
      "episode: 460/5000, time: 79, acton: 1, reward: -89\n",
      "episode: 460/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 460/5000, time: 199, acton: 1, reward: -224\n",
      "episode: 470/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 470/5000, time: 79, acton: 1, reward: -64\n",
      "episode: 470/5000, time: 119, acton: 1, reward: -107\n",
      "episode: 470/5000, time: 139, acton: 1, reward: -138\n",
      "episode: 470/5000, time: 159, acton: 1, reward: -171\n",
      "episode: 470/5000, time: 179, acton: 1, reward: -164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 480/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 480/5000, time: 59, acton: 1, reward: -48\n",
      "episode: 480/5000, time: 79, acton: 1, reward: -83\n",
      "episode: 480/5000, time: 119, acton: 1, reward: -111\n",
      "episode: 480/5000, time: 139, acton: 1, reward: -152\n",
      "episode: 480/5000, time: 159, acton: 1, reward: -184\n",
      "episode: 480/5000, time: 179, acton: 1, reward: -219\n",
      "episode: 480/5000, time: 199, acton: 1, reward: -254\n",
      "episode: 490/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 490/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 490/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 490/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 490/5000, time: 119, acton: 1, reward: -138\n",
      "episode: 490/5000, time: 159, acton: 1, reward: -178\n",
      "episode: 490/5000, time: 179, acton: 1, reward: -210\n",
      "episode: 490/5000, time: 199, acton: 1, reward: -244\n",
      "episode: 500/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 500/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 500/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 500/5000, time: 119, acton: 1, reward: -122\n",
      "episode: 500/5000, time: 139, acton: 1, reward: -155\n",
      "episode: 500/5000, time: 159, acton: 1, reward: -188\n",
      "episode: 500/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 510/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 510/5000, time: 79, acton: 1, reward: -77\n",
      "episode: 510/5000, time: 99, acton: 1, reward: -111\n",
      "episode: 510/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 510/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 510/5000, time: 159, acton: 1, reward: -190\n",
      "episode: 510/5000, time: 179, acton: 1, reward: -197\n",
      "episode: 520/5000, time: 19, acton: 5, reward: 0\n",
      "episode: 520/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 520/5000, time: 79, acton: 1, reward: -75\n",
      "episode: 520/5000, time: 99, acton: 1, reward: -109\n",
      "episode: 520/5000, time: 119, acton: 1, reward: -111\n",
      "episode: 520/5000, time: 139, acton: 1, reward: -150\n",
      "episode: 520/5000, time: 159, acton: 1, reward: -184\n",
      "episode: 520/5000, time: 179, acton: 1, reward: -219\n",
      "episode: 530/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 530/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 530/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 530/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 530/5000, time: 119, acton: 1, reward: -134\n",
      "episode: 530/5000, time: 139, acton: 1, reward: -167\n",
      "episode: 530/5000, time: 159, acton: 1, reward: -199\n",
      "episode: 530/5000, time: 199, acton: 1, reward: -229\n",
      "episode: 540/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 540/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 540/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 540/5000, time: 119, acton: 1, reward: -132\n",
      "episode: 540/5000, time: 139, acton: 1, reward: -147\n",
      "episode: 540/5000, time: 159, acton: 1, reward: -180\n",
      "episode: 540/5000, time: 179, acton: 6, reward: -193\n",
      "episode: 540/5000, time: 199, acton: 1, reward: -214\n",
      "episode: 550/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 550/5000, time: 59, acton: 3, reward: -42\n",
      "episode: 550/5000, time: 79, acton: 1, reward: -73\n",
      "episode: 550/5000, time: 99, acton: 1, reward: -99\n",
      "episode: 550/5000, time: 119, acton: 1, reward: -132\n",
      "episode: 550/5000, time: 179, acton: 1, reward: -204\n",
      "episode: 560/5000, time: 59, acton: 6, reward: -41\n",
      "episode: 560/5000, time: 99, acton: 1, reward: -96\n",
      "episode: 560/5000, time: 199, acton: 1, reward: -208\n",
      "episode: 570/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 570/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 570/5000, time: 79, acton: 1, reward: -82\n",
      "episode: 570/5000, time: 99, acton: 1, reward: -116\n",
      "episode: 570/5000, time: 119, acton: 1, reward: -148\n",
      "episode: 570/5000, time: 139, acton: 1, reward: -140\n",
      "episode: 570/5000, time: 159, acton: 1, reward: -184\n",
      "episode: 570/5000, time: 199, acton: 1, reward: -225\n",
      "episode: 580/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 580/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 580/5000, time: 59, acton: 1, reward: -59\n",
      "episode: 580/5000, time: 79, acton: 1, reward: -94\n",
      "episode: 580/5000, time: 99, acton: 1, reward: -92\n",
      "episode: 580/5000, time: 179, acton: 1, reward: -180\n",
      "episode: 580/5000, time: 199, acton: 1, reward: -205\n",
      "episode: 590/5000, time: 79, acton: 1, reward: -70\n",
      "episode: 590/5000, time: 119, acton: 1, reward: -106\n",
      "episode: 590/5000, time: 159, acton: 1, reward: -161\n",
      "episode: 590/5000, time: 199, acton: 1, reward: -213\n",
      "episode: 600/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 600/5000, time: 59, acton: 1, reward: -64\n",
      "episode: 600/5000, time: 99, acton: 1, reward: -107\n",
      "episode: 600/5000, time: 119, acton: 1, reward: -141\n",
      "episode: 600/5000, time: 139, acton: 1, reward: -175\n",
      "episode: 600/5000, time: 159, acton: 1, reward: -152\n",
      "episode: 600/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 610/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 610/5000, time: 59, acton: 1, reward: -45\n",
      "episode: 610/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 610/5000, time: 159, acton: 1, reward: -172\n",
      "episode: 610/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 620/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 620/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 620/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 620/5000, time: 119, acton: 1, reward: -117\n",
      "episode: 620/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 620/5000, time: 199, acton: 1, reward: -222\n",
      "episode: 630/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 630/5000, time: 79, acton: 1, reward: -75\n",
      "episode: 630/5000, time: 99, acton: 1, reward: -107\n",
      "episode: 630/5000, time: 119, acton: 1, reward: -142\n",
      "episode: 630/5000, time: 139, acton: 1, reward: -176\n",
      "episode: 630/5000, time: 159, acton: 1, reward: -156\n",
      "episode: 630/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 630/5000, time: 199, acton: 1, reward: -239\n",
      "episode: 640/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 640/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 640/5000, time: 139, acton: 1, reward: -133\n",
      "episode: 640/5000, time: 199, acton: 1, reward: -212\n",
      "episode: 650/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 650/5000, time: 59, acton: 1, reward: -48\n",
      "episode: 650/5000, time: 99, acton: 1, reward: -98\n",
      "episode: 650/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 650/5000, time: 139, acton: 1, reward: -147\n",
      "episode: 650/5000, time: 159, acton: 1, reward: -177\n",
      "episode: 650/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 650/5000, time: 199, acton: 1, reward: -197\n",
      "episode: 660/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 660/5000, time: 59, acton: 1, reward: -45\n",
      "episode: 660/5000, time: 79, acton: 1, reward: -80\n",
      "episode: 660/5000, time: 99, acton: 1, reward: -113\n",
      "episode: 660/5000, time: 159, acton: 1, reward: -174\n",
      "episode: 670/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 670/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 670/5000, time: 59, acton: 1, reward: -57\n",
      "episode: 670/5000, time: 79, acton: 1, reward: -92\n",
      "episode: 670/5000, time: 119, acton: 1, reward: -114\n",
      "episode: 670/5000, time: 139, acton: 1, reward: -153\n",
      "episode: 670/5000, time: 179, acton: 1, reward: -196\n",
      "episode: 670/5000, time: 199, acton: 1, reward: -206\n",
      "episode: 680/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 680/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 680/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 680/5000, time: 139, acton: 1, reward: -120\n",
      "episode: 680/5000, time: 159, acton: 1, reward: -143\n",
      "episode: 680/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 680/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 690/5000, time: 79, acton: 1, reward: -71\n",
      "episode: 690/5000, time: 99, acton: 1, reward: -91\n",
      "episode: 690/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 690/5000, time: 179, acton: 1, reward: -200\n",
      "episode: 690/5000, time: 199, acton: 1, reward: -210\n",
      "episode: 700/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 700/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 700/5000, time: 119, acton: 1, reward: -133\n",
      "episode: 700/5000, time: 139, acton: 7, reward: -150\n",
      "episode: 700/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 700/5000, time: 199, acton: 1, reward: -240\n",
      "episode: 710/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 710/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 710/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 710/5000, time: 119, acton: 1, reward: -133\n",
      "episode: 710/5000, time: 139, acton: 1, reward: -166\n",
      "episode: 710/5000, time: 179, acton: 1, reward: -209\n",
      "episode: 720/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 720/5000, time: 39, acton: 1, reward: -24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 720/5000, time: 59, acton: 1, reward: -46\n",
      "episode: 720/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 720/5000, time: 99, acton: 1, reward: -99\n",
      "episode: 720/5000, time: 119, acton: 1, reward: -131\n",
      "episode: 720/5000, time: 179, acton: 1, reward: -177\n",
      "episode: 720/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 730/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 730/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 730/5000, time: 79, acton: 1, reward: -76\n",
      "episode: 730/5000, time: 99, acton: 1, reward: -110\n",
      "episode: 730/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 730/5000, time: 139, acton: 1, reward: -158\n",
      "episode: 730/5000, time: 159, acton: 1, reward: -192\n",
      "episode: 730/5000, time: 179, acton: 1, reward: -225\n",
      "episode: 730/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 740/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 740/5000, time: 59, acton: 5, reward: -48\n",
      "episode: 740/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 740/5000, time: 139, acton: 1, reward: -154\n",
      "episode: 740/5000, time: 159, acton: 1, reward: -186\n",
      "episode: 750/5000, time: 39, acton: 1, reward: -18\n",
      "episode: 750/5000, time: 59, acton: 1, reward: -50\n",
      "episode: 750/5000, time: 79, acton: 1, reward: -85\n",
      "episode: 750/5000, time: 99, acton: 1, reward: -119\n",
      "episode: 750/5000, time: 119, acton: 1, reward: -152\n",
      "episode: 750/5000, time: 139, acton: 1, reward: -185\n",
      "episode: 750/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 750/5000, time: 199, acton: 1, reward: -205\n",
      "episode: 760/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 760/5000, time: 59, acton: 1, reward: -46\n",
      "episode: 760/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 760/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 760/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 770/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 770/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 770/5000, time: 99, acton: 1, reward: -92\n",
      "episode: 770/5000, time: 139, acton: 1, reward: -139\n",
      "episode: 770/5000, time: 159, acton: 1, reward: -174\n",
      "episode: 770/5000, time: 179, acton: 1, reward: -207\n",
      "episode: 780/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 780/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 780/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 780/5000, time: 79, acton: 1, reward: -101\n",
      "episode: 780/5000, time: 99, acton: 1, reward: -136\n",
      "episode: 780/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 780/5000, time: 159, acton: 1, reward: -188\n",
      "episode: 780/5000, time: 179, acton: 1, reward: -194\n",
      "episode: 780/5000, time: 199, acton: 1, reward: -234\n",
      "episode: 790/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 790/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 790/5000, time: 79, acton: 1, reward: -72\n",
      "episode: 790/5000, time: 99, acton: 6, reward: -91\n",
      "episode: 790/5000, time: 119, acton: 1, reward: -116\n",
      "episode: 790/5000, time: 159, acton: 1, reward: -165\n",
      "episode: 790/5000, time: 179, acton: 7, reward: -188\n",
      "episode: 790/5000, time: 199, acton: 1, reward: -200\n",
      "episode: 800/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 800/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 800/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 800/5000, time: 79, acton: 1, reward: -77\n",
      "episode: 800/5000, time: 99, acton: 1, reward: -103\n",
      "episode: 800/5000, time: 119, acton: 1, reward: -136\n",
      "episode: 800/5000, time: 199, acton: 1, reward: -226\n",
      "episode: 810/5000, time: 19, acton: 5, reward: 0\n",
      "episode: 810/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 810/5000, time: 119, acton: 6, reward: -118\n",
      "episode: 810/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 810/5000, time: 199, acton: 1, reward: -206\n",
      "episode: 820/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 820/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 820/5000, time: 79, acton: 1, reward: -87\n",
      "episode: 820/5000, time: 99, acton: 1, reward: -118\n",
      "episode: 820/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 820/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 820/5000, time: 199, acton: 1, reward: -242\n",
      "episode: 830/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 830/5000, time: 59, acton: 1, reward: -62\n",
      "episode: 830/5000, time: 79, acton: 1, reward: -70\n",
      "episode: 830/5000, time: 99, acton: 1, reward: -89\n",
      "episode: 830/5000, time: 159, acton: 1, reward: -159\n",
      "episode: 830/5000, time: 179, acton: 1, reward: -183\n",
      "episode: 830/5000, time: 199, acton: 1, reward: -209\n",
      "episode: 840/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 840/5000, time: 59, acton: 1, reward: -42\n",
      "episode: 840/5000, time: 79, acton: 1, reward: -69\n",
      "episode: 840/5000, time: 99, acton: 1, reward: -104\n",
      "episode: 840/5000, time: 139, acton: 1, reward: -148\n",
      "episode: 840/5000, time: 159, acton: 1, reward: -182\n",
      "episode: 840/5000, time: 179, acton: 1, reward: -190\n",
      "episode: 840/5000, time: 199, acton: 1, reward: -217\n",
      "episode: 850/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 850/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 850/5000, time: 59, acton: 1, reward: -56\n",
      "episode: 850/5000, time: 79, acton: 1, reward: -91\n",
      "episode: 850/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 850/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 850/5000, time: 199, acton: 1, reward: -220\n",
      "episode: 860/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 860/5000, time: 59, acton: 1, reward: -44\n",
      "episode: 860/5000, time: 79, acton: 1, reward: -79\n",
      "episode: 860/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 860/5000, time: 139, acton: 1, reward: -155\n",
      "episode: 860/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 870/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 870/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 870/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 870/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 870/5000, time: 139, acton: 1, reward: -154\n",
      "episode: 870/5000, time: 159, acton: 1, reward: -187\n",
      "episode: 870/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 870/5000, time: 199, acton: 1, reward: -219\n",
      "episode: 880/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 880/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 880/5000, time: 59, acton: 1, reward: -36\n",
      "episode: 880/5000, time: 79, acton: 1, reward: -74\n",
      "episode: 880/5000, time: 99, acton: 6, reward: -89\n",
      "episode: 880/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 880/5000, time: 139, acton: 1, reward: -138\n",
      "episode: 880/5000, time: 159, acton: 1, reward: -170\n",
      "episode: 880/5000, time: 179, acton: 1, reward: -204\n",
      "episode: 890/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 890/5000, time: 39, acton: 6, reward: -16\n",
      "episode: 890/5000, time: 59, acton: 1, reward: -43\n",
      "episode: 890/5000, time: 79, acton: 1, reward: -75\n",
      "episode: 890/5000, time: 99, acton: 1, reward: -95\n",
      "episode: 890/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 890/5000, time: 139, acton: 1, reward: -160\n",
      "episode: 890/5000, time: 179, acton: 1, reward: -198\n",
      "episode: 890/5000, time: 199, acton: 1, reward: -231\n",
      "episode: 900/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 900/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 900/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 900/5000, time: 119, acton: 1, reward: -120\n",
      "episode: 900/5000, time: 139, acton: 1, reward: -152\n",
      "episode: 900/5000, time: 159, acton: 1, reward: -186\n",
      "episode: 900/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 900/5000, time: 199, acton: 1, reward: -222\n",
      "episode: 910/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 910/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 910/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 910/5000, time: 79, acton: 1, reward: -89\n",
      "episode: 910/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 910/5000, time: 159, acton: 1, reward: -161\n",
      "episode: 910/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 920/5000, time: 19, acton: 7, reward: 0\n",
      "episode: 920/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 920/5000, time: 119, acton: 1, reward: -113\n",
      "episode: 920/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 930/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 930/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 930/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 930/5000, time: 119, acton: 1, reward: -121\n",
      "episode: 930/5000, time: 159, acton: 1, reward: -169\n",
      "episode: 930/5000, time: 179, acton: 1, reward: -202\n",
      "episode: 930/5000, time: 199, acton: 1, reward: -237\n",
      "episode: 940/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 940/5000, time: 39, acton: 1, reward: -22\n",
      "episode: 940/5000, time: 99, acton: 1, reward: -89\n",
      "episode: 940/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 940/5000, time: 139, acton: 7, reward: -139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 940/5000, time: 179, acton: 5, reward: -190\n",
      "episode: 940/5000, time: 199, acton: 1, reward: -193\n",
      "episode: 950/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 950/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 950/5000, time: 59, acton: 1, reward: -43\n",
      "episode: 950/5000, time: 99, acton: 5, reward: -88\n",
      "episode: 950/5000, time: 119, acton: 1, reward: -111\n",
      "episode: 950/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 950/5000, time: 179, acton: 1, reward: -172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-b9d0f636450d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 所有车辆的平均行驶时间，除以总时间后越大则越好，[0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#         average_travel_time = eng.get_average_travel_time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-48d4bd3998f4>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_expected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_critic_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "for i in range(args.max_episode):\n",
    "    env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    s = env.get_state()\n",
    "    s = np.array(list(s['start_lane_vehicle_count'].values()) + \n",
    "                 [s['current_phase']])\n",
    "    s = np.reshape(s, [1, s_dim])\n",
    "    s = s.astype(np.float32)\n",
    "#     s = torch.tensor(s)\n",
    "#     print(trainer.choose_action(s))\n",
    "    last_action = phase_list[int(trainer.choose_action(torch.tensor(s)))]\n",
    "    \n",
    "    while t < args.max_step:\n",
    "        a_choice = trainer.choose_action(torch.tensor(s))\n",
    "        a = phase_list[int(a_choice)]\n",
    "            \n",
    "        if a == last_action:\n",
    "            env.step(a)\n",
    "        else:\n",
    "            for _ in range(env.yellow_time):\n",
    "                env.step(0)\n",
    "                t += 1\n",
    "                flag = (t >= args.max_step)\n",
    "                if flag:\n",
    "                    break\n",
    "            if flag:\n",
    "                break\n",
    "            env.step(a)\n",
    "            \n",
    "        last_action = a\n",
    "        t += 1\n",
    "        next_state = env.get_state()\n",
    "        r = env.get_reward()\n",
    "        next_state = np.array(list(next_state['start_lane_vehicle_count'].values()) + \n",
    "                              [next_state['current_phase']])\n",
    "        next_state = np.reshape(next_state, [1, s_dim])\n",
    "        next_state = next_state.astype(np.float32)\n",
    "#         next_state = torch.tensor(next_state)\n",
    "        \n",
    "        trainer.replay_buffer.add(s, a_choice, r, next_state)\n",
    "        s = next_state\n",
    "        \n",
    "        total_time = t + i * args.max_step\n",
    "        if total_time > args.learning_start and total_time % args.update_freq == 0:\n",
    "            trainer.optimize()\n",
    "        # 所有车辆的平均行驶时间，除以总时间后越大则越好，[0,1]\n",
    "#         average_travel_time = eng.get_average_travel_time()\n",
    "#         reward_travel_time = eng.get_current_time()/average_travel_time\n",
    "        \n",
    "        if i % 10 == 0 and t % 20 == 0:\n",
    "            print(\"episode: {}/{}, time: {}, acton: {}, reward: {}\"\n",
    "              .format(i, args.max_episode, t-1, a, r))\n",
    "\n",
    "#     if i%100 == 0:\n",
    "#         trainer.save_models(i)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
