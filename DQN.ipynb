{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "      \n",
    "    #RL的参数\n",
    "    parser.add_argument('--bs', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.003, help=\"lr decay\")\n",
    "    parser.add_argument('--tau', type=float, default=0.001)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "\n",
    "    #训练参数\n",
    "    parser.add_argument('--max_episode', type=int, default=5000)\n",
    "    parser.add_argument('--max_step', type=int, default=3600)\n",
    "    parser.add_argument('--max_buffer', type=int, default=10000)\n",
    "    parser.add_argument('--max_total_reward', type=float)\n",
    "    parser.add_argument('--epsilon', type=float, default=0.95)\n",
    "    parser.add_argument('--learning_start', type=int, default=600)\n",
    "    parser.add_argument('--update_freq', type=int, default=300)\n",
    "    parser.add_argument('--update_target_freq', type=int, default=1500)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "from itertools import count\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "import cityflow\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "args = args_parser()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityFlowEnv():\n",
    "    '''\n",
    "    Simulator Environment with CityFlow\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        self.env = cityflow.Engine(config_file='examples/config_control.json', thread_num=1)\n",
    "#         self.eng.load_roadnet(config['roadnet'])\n",
    "#         self.eng.load_flow(config['flow'])\n",
    "#         self.config = config\n",
    "        self.num_step = args.max_step\n",
    "        self.lane_phase_info = self.parse_roadnet('examples/roadnet.json') # \"intersection_1_1\"\n",
    "\n",
    "        self.intersection_id = list(self.lane_phase_info.keys())[0]\n",
    "        self.start_lane = self.lane_phase_info[self.intersection_id]['start_lane']\n",
    "        self.phase_list = self.lane_phase_info[self.intersection_id][\"phase\"]\n",
    "        self.phase_startLane_mapping = self.lane_phase_info[self.intersection_id][\"phase_startLane_mapping\"]\n",
    "\n",
    "        self.current_phase = self.phase_list[0]\n",
    "        self.current_phase_time = 0\n",
    "        self.yellow_time = 5\n",
    "\n",
    "        self.phase_log = []\n",
    "\n",
    "    def parse_roadnet(self, roadnetFile):\n",
    "        roadnet = json.load(open(roadnetFile))\n",
    "        lane_phase_info_dict ={}\n",
    "\n",
    "        # many intersections exist in the roadnet and virtual intersection is controlled by signal\n",
    "        for intersection in roadnet[\"intersections\"]:\n",
    "            if intersection['virtual']:\n",
    "                continue\n",
    "            lane_phase_info_dict[intersection['id']] = {\"start_lane\": [],\n",
    "                                                         \"end_lane\": [],\n",
    "                                                         \"phase\": [],\n",
    "                                                         \"phase_startLane_mapping\": {},\n",
    "                                                         \"phase_roadLink_mapping\": {}}\n",
    "            road_links = intersection[\"roadLinks\"]\n",
    "\n",
    "            start_lane = []\n",
    "            end_lane = []\n",
    "            roadLink_lane_pair = {ri: [] for ri in\n",
    "                                  range(len(road_links))}  # roadLink includes some lane_pair: (start_lane, end_lane)\n",
    "\n",
    "            for ri in range(len(road_links)):\n",
    "                road_link = road_links[ri]\n",
    "                for lane_link in road_link[\"laneLinks\"]:\n",
    "                    sl = road_link['startRoad'] + \"_\" + str(lane_link[\"startLaneIndex\"])\n",
    "                    el = road_link['endRoad'] + \"_\" + str(lane_link[\"endLaneIndex\"])\n",
    "                    start_lane.append(sl)\n",
    "                    end_lane.append(el)\n",
    "                    roadLink_lane_pair[ri].append((sl, el))\n",
    "\n",
    "            lane_phase_info_dict[intersection['id']][\"start_lane\"] = sorted(list(set(start_lane)))\n",
    "            lane_phase_info_dict[intersection['id']][\"end_lane\"] = sorted(list(set(end_lane)))\n",
    "\n",
    "            for phase_i in range(1, len(intersection[\"trafficLight\"][\"lightphases\"])):\n",
    "                p = intersection[\"trafficLight\"][\"lightphases\"][phase_i]\n",
    "                lane_pair = []\n",
    "                start_lane = []\n",
    "                for ri in p[\"availableRoadLinks\"]:\n",
    "                    lane_pair.extend(roadLink_lane_pair[ri])\n",
    "                    if roadLink_lane_pair[ri][0][0] not in start_lane:\n",
    "                        start_lane.append(roadLink_lane_pair[ri][0][0])\n",
    "                lane_phase_info_dict[intersection['id']][\"phase\"].append(phase_i)\n",
    "                lane_phase_info_dict[intersection['id']][\"phase_startLane_mapping\"][phase_i] = start_lane\n",
    "                lane_phase_info_dict[intersection['id']][\"phase_roadLink_mapping\"][phase_i] = lane_pair\n",
    "\n",
    "        return lane_phase_info_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.phase_log = []\n",
    "\n",
    "    def step(self, next_phase):\n",
    "        if self.current_phase == next_phase:\n",
    "            self.current_phase_time += 1\n",
    "        else:\n",
    "            self.current_phase = next_phase\n",
    "            self.current_phase_time = 1\n",
    "\n",
    "        self.env.set_tl_phase(self.intersection_id, self.current_phase)\n",
    "        self.env.next_step()\n",
    "        self.phase_log.append(self.current_phase)\n",
    "\n",
    "    def get_state(self):\n",
    "        state = {}\n",
    "        state['lane_vehicle_count'] = self.env.get_lane_vehicle_count()  # {lane_id: lane_count, ...}\n",
    "        state['start_lane_vehicle_count'] = {lane: self.env.get_lane_vehicle_count()[lane] for lane in self.start_lane}\n",
    "        state['lane_waiting_vehicle_count'] = self.env.get_lane_waiting_vehicle_count()  # {lane_id: lane_waiting_count, ...}\n",
    "        state['lane_vehicles'] = self.env.get_lane_vehicles()  # {lane_id: [vehicle1_id, vehicle2_id, ...], ...}\n",
    "        state['vehicle_speed'] = self.env.get_vehicle_speed()  # {vehicle_id: vehicle_speed, ...}\n",
    "        state['vehicle_distance'] = self.env.get_vehicle_distance() # {vehicle_id: distance, ...}\n",
    "        state['current_time'] = self.env.get_current_time()\n",
    "        state['current_phase'] = self.current_phase\n",
    "        state['current_phase_time'] = self.current_phase_time\n",
    "\n",
    "        return state\n",
    "\n",
    "    def get_reward(self):\n",
    "        # a sample reward function which calculates the total of waiting vehicles\n",
    "        lane_waiting_vehicle_count = self.env.get_lane_waiting_vehicle_count()\n",
    "        reward = -1 * sum(list(lane_waiting_vehicle_count.values()))\n",
    "        return reward\n",
    "\n",
    "    def log(self):\n",
    "        #self.eng.print_log(self.config['replay_data_path'] + \"/replay_roadnet.json\",\n",
    "        #                   self.config['replay_data_path'] + \"/replay_flow.json\")\n",
    "        df = pd.DataFrame({self.intersection_id: self.phase_log[:self.num_step]})\n",
    "        if not os.path.exists(self.config['data']):\n",
    "            os.makedirs(self.config[\"data\"])\n",
    "        df.to_csv(os.path.join(self.config['data'], 'signal_plan_template.txt'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, state_dim, action_dim, lane_phase_info, args):\n",
    "        self.args = args\n",
    "        self.state_size = state_dim\n",
    "        self.action_size = action_dim\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.update_target_freq = 5\n",
    "        self.batch_size = 30\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_network()\n",
    "\n",
    "        intersection_id = list(lane_phase_info.keys())[0]\n",
    "        self.phase_list = lane_phase_info[intersection_id]['phase']\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(40, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(40, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        action = self.phase_list.index(action)\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self):\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            target = (reward + self.gamma *\n",
    "                      np.amax(self.target_model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityFlowEnv(args)\n",
    "\n",
    "lane_phase_info = env.lane_phase_info\n",
    "intersection_id = list(lane_phase_info.keys())[0]\n",
    "phase_list = lane_phase_info[intersection_id]['phase']\n",
    "\n",
    "s_dim = len(lane_phase_info[intersection_id]['start_lane']) + 1\n",
    "a_dim = len(phase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(s_dim, a_dim, lane_phase_info, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, time: 2399, acton: 5, reward: -414\n",
      "episode: 0/5000, time: 3599, acton: 2, reward: -203\n",
      "episode: 10/5000, time: 1199, acton: 2, reward: -203\n",
      "episode: 10/5000, time: 3599, acton: 2, reward: -297\n",
      "episode: 30/5000, time: 3599, acton: 4, reward: -194\n",
      "episode: 50/5000, time: 2399, acton: 6, reward: -198\n",
      "episode: 60/5000, time: 2399, acton: 4, reward: -272\n",
      "episode: 70/5000, time: 1199, acton: 6, reward: -255\n",
      "episode: 70/5000, time: 3599, acton: 3, reward: -361\n",
      "episode: 80/5000, time: 2399, acton: 2, reward: -280\n",
      "episode: 80/5000, time: 3599, acton: 2, reward: -319\n",
      "episode: 90/5000, time: 1199, acton: 6, reward: -330\n",
      "episode: 90/5000, time: 2399, acton: 6, reward: -319\n",
      "episode: 100/5000, time: 2399, acton: 2, reward: -179\n",
      "episode: 120/5000, time: 1199, acton: 4, reward: -303\n",
      "episode: 120/5000, time: 2399, acton: 4, reward: -226\n",
      "episode: 120/5000, time: 3599, acton: 4, reward: -260\n",
      "episode: 130/5000, time: 3599, acton: 4, reward: -236\n",
      "episode: 140/5000, time: 1199, acton: 5, reward: -176\n",
      "episode: 140/5000, time: 3599, acton: 2, reward: -188\n",
      "episode: 150/5000, time: 1199, acton: 6, reward: -167\n",
      "episode: 160/5000, time: 1199, acton: 2, reward: -195\n",
      "episode: 160/5000, time: 2399, acton: 4, reward: -212\n",
      "episode: 170/5000, time: 2399, acton: 4, reward: -249\n",
      "episode: 190/5000, time: 1199, acton: 4, reward: -252\n",
      "episode: 190/5000, time: 2399, acton: 4, reward: -187\n",
      "episode: 200/5000, time: 1199, acton: 2, reward: -194\n",
      "episode: 210/5000, time: 1199, acton: 6, reward: -331\n",
      "episode: 210/5000, time: 2399, acton: 6, reward: -311\n",
      "episode: 210/5000, time: 3599, acton: 4, reward: -306\n",
      "episode: 230/5000, time: 1199, acton: 6, reward: -345\n",
      "episode: 240/5000, time: 3599, acton: 6, reward: -219\n",
      "episode: 260/5000, time: 1199, acton: 6, reward: -178\n",
      "episode: 260/5000, time: 3599, acton: 6, reward: -299\n",
      "episode: 270/5000, time: 1199, acton: 2, reward: -225\n",
      "episode: 270/5000, time: 2399, acton: 2, reward: -252\n",
      "episode: 270/5000, time: 3599, acton: 2, reward: -286\n",
      "episode: 280/5000, time: 1199, acton: 4, reward: -270\n",
      "episode: 280/5000, time: 2399, acton: 2, reward: -213\n",
      "episode: 280/5000, time: 3599, acton: 6, reward: -368\n",
      "episode: 290/5000, time: 1199, acton: 6, reward: -236\n",
      "episode: 300/5000, time: 1199, acton: 6, reward: -248\n",
      "episode: 300/5000, time: 2399, acton: 6, reward: -231\n",
      "episode: 310/5000, time: 1199, acton: 2, reward: -212\n",
      "episode: 320/5000, time: 1199, acton: 2, reward: -231\n",
      "episode: 330/5000, time: 3599, acton: 2, reward: -375\n",
      "episode: 340/5000, time: 2399, acton: 6, reward: -278\n",
      "episode: 350/5000, time: 2399, acton: 4, reward: -228\n",
      "episode: 360/5000, time: 1199, acton: 6, reward: -191\n",
      "episode: 370/5000, time: 1199, acton: 4, reward: -160\n",
      "episode: 380/5000, time: 1199, acton: 6, reward: -234\n",
      "episode: 380/5000, time: 2399, acton: 2, reward: -249\n",
      "episode: 380/5000, time: 3599, acton: 4, reward: -243\n",
      "episode: 390/5000, time: 2399, acton: 6, reward: -285\n",
      "episode: 390/5000, time: 3599, acton: 4, reward: -346\n",
      "episode: 400/5000, time: 3599, acton: 2, reward: -355\n",
      "episode: 410/5000, time: 1199, acton: 2, reward: -435\n",
      "episode: 410/5000, time: 2399, acton: 2, reward: -382\n",
      "episode: 420/5000, time: 2399, acton: 2, reward: -281\n",
      "episode: 430/5000, time: 1199, acton: 6, reward: -250\n",
      "episode: 430/5000, time: 2399, acton: 6, reward: -234\n"
     ]
    }
   ],
   "source": [
    "for i in range(args.max_episode):\n",
    "    env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    s = env.get_state()\n",
    "    s = np.array(list(s['start_lane_vehicle_count'].values()) + \n",
    "                 [s['current_phase']])\n",
    "    s = np.reshape(s, [1, s_dim])\n",
    "    s = s.astype(np.float32)\n",
    "#     s = torch.tensor(s)\n",
    "#     print(trainer.choose_action(s))\n",
    "#     last_action = phase_list[int(trainer.choose_action(torch.tensor(s)))]\n",
    "    last_action = phase_list[trainer.choose_action(s)]\n",
    "    \n",
    "    while t < args.max_step:\n",
    "#         a_choice = trainer.choose_action(torch.tensor(s))\n",
    "#         a = phase_list[int(a_choice)]\n",
    "\n",
    "        a = phase_list[trainer.choose_action(s)]\n",
    "            \n",
    "        if a == last_action:\n",
    "            env.step(a)\n",
    "        else:\n",
    "            for _ in range(env.yellow_time):\n",
    "                env.step(0)\n",
    "                t += 1\n",
    "                flag = (t >= args.max_step)\n",
    "                if flag:\n",
    "                    break\n",
    "            if flag:\n",
    "                break\n",
    "            env.step(a)\n",
    "            \n",
    "        last_action = a\n",
    "        t += 1\n",
    "        next_state = env.get_state()\n",
    "        r = env.get_reward()\n",
    "        next_state = np.array(list(next_state['start_lane_vehicle_count'].values()) + \n",
    "                              [next_state['current_phase']])\n",
    "        next_state = np.reshape(next_state, [1, s_dim])\n",
    "        next_state = next_state.astype(np.float32)\n",
    "#         next_state = torch.tensor(next_state)\n",
    "        \n",
    "        trainer.remember(s, a, r, next_state)\n",
    "        s = next_state\n",
    "        \n",
    "        total_time = t + i * args.max_step\n",
    "        if total_time > args.learning_start and total_time % args.update_freq == 0:\n",
    "            trainer.replay()\n",
    "        if total_time > args.learning_start and total_time % args.update_target_freq == 0:\n",
    "            trainer.replay()\n",
    "        # 所有车辆的平均行驶时间，除以总时间后越大则越好，[0,1]\n",
    "#         average_travel_time = eng.get_average_travel_time()\n",
    "#         reward_travel_time = eng.get_current_time()/average_travel_time\n",
    "        \n",
    "        if i % 10 == 0 and t % 1200 == 0:\n",
    "            print(\"episode: {}/{}, time: {}, acton: {}, reward: {}\"\n",
    "              .format(i, args.max_episode, t-1, a, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save('DQN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
