{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "      \n",
    "    #RL的参数\n",
    "    parser.add_argument('--bs', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.003, help=\"lr decay\")\n",
    "    parser.add_argument('--tau', type=float, default=0.001)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "\n",
    "    #训练参数\n",
    "    parser.add_argument('--max_episode', type=int, default=5000)\n",
    "    parser.add_argument('--max_step', type=int, default=3600)\n",
    "    parser.add_argument('--max_buffer', type=int, default=10000)\n",
    "    parser.add_argument('--max_total_reward', type=float)\n",
    "    parser.add_argument('--epsilon', type=float, default=0.95)\n",
    "    parser.add_argument('--learning_start', type=int, default=600)\n",
    "    parser.add_argument('--update_freq', type=int, default=5)\n",
    "    parser.add_argument('--clip', type=float, default=0.2)\n",
    "    parser.add_argument('--max_epoch', type=int, default=50)\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import count\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from multiprocessing_env import SubprocVecEnv\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from collections import deque\n",
    "import cityflow\n",
    "import json\n",
    "\n",
    "args = args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "\n",
    "    def sample(self, count):\n",
    "        \"\"\"\n",
    "        samples a random batch from the replay memory buffer\n",
    "        :param count: batch size\n",
    "        :return: batch (numpy array)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "#         print(batch)\n",
    "        s_arr = np.float32([arr[0] for arr in batch])\n",
    "        a_arr = np.float32([arr[1] for arr in batch])\n",
    "        r_arr = np.float32([arr[2] for arr in batch])\n",
    "        s1_arr = np.float32([arr[3] for arr in batch])\n",
    "\n",
    "        return s_arr, a_arr, r_arr, s1_arr\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def add(self, s, a, r, s1):\n",
    "        \"\"\"\n",
    "        adds a particular transaction in the memory buffer\n",
    "        :param s: current state\n",
    "        :param a: action taken\n",
    "        :param r: reward received\n",
    "        :param s1: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        transition = (s,a,r,s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityFlowEnv():\n",
    "    '''\n",
    "    Simulator Environment with CityFlow\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        self.env = cityflow.Engine(config_file='examples/config_control.json', thread_num=1)\n",
    "#         self.eng.load_roadnet(config['roadnet'])\n",
    "#         self.eng.load_flow(config['flow'])\n",
    "#         self.config = config\n",
    "        self.num_step = args.max_step\n",
    "        self.lane_phase_info = self.parse_roadnet('examples/roadnet.json') # \"intersection_1_1\"\n",
    "\n",
    "        self.intersection_id = list(self.lane_phase_info.keys())[0]\n",
    "        self.start_lane = self.lane_phase_info[self.intersection_id]['start_lane']\n",
    "        self.phase_list = self.lane_phase_info[self.intersection_id][\"phase\"]\n",
    "        self.phase_startLane_mapping = self.lane_phase_info[self.intersection_id][\"phase_startLane_mapping\"]\n",
    "\n",
    "        self.current_phase = self.phase_list[0]\n",
    "        self.current_phase_time = 0\n",
    "        self.yellow_time = 5\n",
    "\n",
    "        self.phase_log = []\n",
    "\n",
    "    def parse_roadnet(self, roadnetFile):\n",
    "        roadnet = json.load(open(roadnetFile))\n",
    "        lane_phase_info_dict ={}\n",
    "\n",
    "        # many intersections exist in the roadnet and virtual intersection is controlled by signal\n",
    "        for intersection in roadnet[\"intersections\"]:\n",
    "            if intersection['virtual']:\n",
    "                continue\n",
    "            lane_phase_info_dict[intersection['id']] = {\"start_lane\": [],\n",
    "                                                         \"end_lane\": [],\n",
    "                                                         \"phase\": [],\n",
    "                                                         \"phase_startLane_mapping\": {},\n",
    "                                                         \"phase_roadLink_mapping\": {}}\n",
    "            road_links = intersection[\"roadLinks\"]\n",
    "\n",
    "            start_lane = []\n",
    "            end_lane = []\n",
    "            roadLink_lane_pair = {ri: [] for ri in\n",
    "                                  range(len(road_links))}  # roadLink includes some lane_pair: (start_lane, end_lane)\n",
    "\n",
    "            for ri in range(len(road_links)):\n",
    "                road_link = road_links[ri]\n",
    "                for lane_link in road_link[\"laneLinks\"]:\n",
    "                    sl = road_link['startRoad'] + \"_\" + str(lane_link[\"startLaneIndex\"])\n",
    "                    el = road_link['endRoad'] + \"_\" + str(lane_link[\"endLaneIndex\"])\n",
    "                    start_lane.append(sl)\n",
    "                    end_lane.append(el)\n",
    "                    roadLink_lane_pair[ri].append((sl, el))\n",
    "\n",
    "            lane_phase_info_dict[intersection['id']][\"start_lane\"] = sorted(list(set(start_lane)))\n",
    "            lane_phase_info_dict[intersection['id']][\"end_lane\"] = sorted(list(set(end_lane)))\n",
    "\n",
    "            for phase_i in range(1, len(intersection[\"trafficLight\"][\"lightphases\"])):\n",
    "                p = intersection[\"trafficLight\"][\"lightphases\"][phase_i]\n",
    "                lane_pair = []\n",
    "                start_lane = []\n",
    "                for ri in p[\"availableRoadLinks\"]:\n",
    "                    lane_pair.extend(roadLink_lane_pair[ri])\n",
    "                    if roadLink_lane_pair[ri][0][0] not in start_lane:\n",
    "                        start_lane.append(roadLink_lane_pair[ri][0][0])\n",
    "                lane_phase_info_dict[intersection['id']][\"phase\"].append(phase_i)\n",
    "                lane_phase_info_dict[intersection['id']][\"phase_startLane_mapping\"][phase_i] = start_lane\n",
    "                lane_phase_info_dict[intersection['id']][\"phase_roadLink_mapping\"][phase_i] = lane_pair\n",
    "\n",
    "        return lane_phase_info_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.phase_log = []\n",
    "\n",
    "    def step(self, next_phase):\n",
    "        if self.current_phase == next_phase:\n",
    "            self.current_phase_time += 1\n",
    "        else:\n",
    "            self.current_phase = next_phase\n",
    "            self.current_phase_time = 1\n",
    "\n",
    "        self.env.set_tl_phase(self.intersection_id, self.current_phase)\n",
    "        self.env.next_step()\n",
    "        self.phase_log.append(self.current_phase)\n",
    "\n",
    "    def get_state(self):\n",
    "        state = {}\n",
    "        state['lane_vehicle_count'] = self.env.get_lane_vehicle_count()  # {lane_id: lane_count, ...}\n",
    "        state['start_lane_vehicle_count'] = {lane: self.env.get_lane_vehicle_count()[lane] for lane in self.start_lane}\n",
    "        state['lane_waiting_vehicle_count'] = self.env.get_lane_waiting_vehicle_count()  # {lane_id: lane_waiting_count, ...}\n",
    "        state['lane_vehicles'] = self.env.get_lane_vehicles()  # {lane_id: [vehicle1_id, vehicle2_id, ...], ...}\n",
    "        state['vehicle_speed'] = self.env.get_vehicle_speed()  # {vehicle_id: vehicle_speed, ...}\n",
    "        state['vehicle_distance'] = self.env.get_vehicle_distance() # {vehicle_id: distance, ...}\n",
    "        state['current_time'] = self.env.get_current_time()\n",
    "        state['current_phase'] = self.current_phase\n",
    "        state['current_phase_time'] = self.current_phase_time\n",
    "\n",
    "        return state\n",
    "\n",
    "    def get_reward(self):\n",
    "        # a sample reward function which calculates the total of waiting vehicles\n",
    "        lane_waiting_vehicle_count = self.env.get_lane_waiting_vehicle_count()\n",
    "        reward = -1 * sum(list(lane_waiting_vehicle_count.values()))\n",
    "        return reward\n",
    "\n",
    "    def log(self):\n",
    "        #self.eng.print_log(self.config['replay_data_path'] + \"/replay_roadnet.json\",\n",
    "        #                   self.config['replay_data_path'] + \"/replay_flow.json\")\n",
    "        df = pd.DataFrame({self.intersection_id: self.phase_log[:self.num_step]})\n",
    "        if not os.path.exists(self.config['data']):\n",
    "            os.makedirs(self.config[\"data\"])\n",
    "        df.to_csv(os.path.join(self.config['data'], 'signal_plan_template.txt'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, args):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_dim),\n",
    "            nn.Softmax(dim=-1)  # For discrete actions, we use softmax policy\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def act(self, state, replay_buffer):       # state (1,8)\n",
    "        action_probs = self.actor(state)        # (1,4)\n",
    "        dist = Categorical(action_probs) # distribution func: sample an action (return the corresponding index) according to the probs \n",
    "        action = dist.sample()          \n",
    "        action_logprob = dist.log_prob(action)  # (1,)\n",
    "\n",
    "#         replay_buffer.states.append(state)\n",
    "#         replay_buffer.actions.append(action)\n",
    "#         replay_buffer.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.item() # convert to scalar\n",
    "\n",
    "#     def evaluate(self, state, action):      # state (2000, 8); action (2000, 4)\n",
    "#         state_value = self.critic(state)    # (2000, 1)\n",
    "\n",
    "#         # to calculate action score(logprobs) and distribution entropy\n",
    "#         action_probs = self.actor(state)    # (2000,4)\n",
    "#         dist = Categorical(action_probs)\n",
    "#         action_logprobs = dist.log_prob(action) # (2000, 1)\n",
    "#         dist_entropy = dist.entropy()\n",
    "\n",
    "#         return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        #delta is Bellman equation minus value of the state\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        #moving average of advantages discounted by gamma * tau\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, replay_buffer, lane_phase_info, args):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.iter = 0\n",
    "        self.loss_critic_save = []\n",
    "        self.loss_actor_save = []\n",
    "        self.args = args\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, args)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.args.lr)\n",
    "        \n",
    "        self.old_policy = ActorCritic(state_dim, action_dim, args)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "        \n",
    "        intersection_id = list(lane_phase_info.keys())[0]\n",
    "        self.phase_list = lane_phase_info[intersection_id]['phase']\n",
    "\n",
    "#     def select_action(self, state, replay_buffer):\n",
    "#         state = torch.FloatTensor(state.reshape(1, -1)).to(device)  # flatten the state\n",
    "#         return self.old_policy.act(state, replay_buffer)\n",
    "\n",
    "#     def choose_action(self, state):\n",
    "#         if np.random.rand() > self.args.epsilon:\n",
    "#             return random.randrange(self.action_dim)\n",
    "#         act_values = self.actor.forward(state)\n",
    "#         return act_values[0]  # returns action\n",
    "\n",
    "    \n",
    "    def optimize(self, replay_buffer):\n",
    "        # Monte Carlo estimation of rewards\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(replay_buffer.rewards):\n",
    "            discounted_reward = reward + self.args.gamma * discounted_reward\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalize rewards\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(replay_buffer.states)).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(replay_buffer.actions)).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(replay_buffer.logprobs)).detach()\n",
    "\n",
    "        # Train policy for K epochs: sampling and updating\n",
    "        for _ in range(self.args.max_epoch):\n",
    "            # Evaluate old actions and values using current policy\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Importance ratio: p/q\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Advantages\n",
    "            advantages = rewards - state_values.detach()  # old states' rewards - old states' value( evaluated by current policy)\n",
    "\n",
    "            # Actor loss using Surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.args.clip, 1 + self.args.clip) * advantages\n",
    "            actor_loss = - torch.min(surr1, surr2)\n",
    "\n",
    "            # Critic loss: critic loss - entropy\n",
    "            critic_loss = 0.5 * self.MSE_loss(rewards, state_values) - 0.01 * dist_entropy\n",
    "\n",
    "            # Total loss\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            # Backward gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights to old_policy\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityFlowEnv(args)\n",
    "\n",
    "lane_phase_info = env.lane_phase_info\n",
    "intersection_id = list(lane_phase_info.keys())[0]\n",
    "phase_list = lane_phase_info[intersection_id]['phase']\n",
    "\n",
    "s_dim = len(lane_phase_info[intersection_id]['start_lane']) + 1\n",
    "a_dim = len(phase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryBuffer()\n",
    "trainer = Trainer(s_dim, a_dim, replay_buffer, lane_phase_info, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, time: 39, acton: 6, reward: -22\n",
      "episode: 0/5000, time: 59, acton: 6, reward: -47\n",
      "episode: 0/5000, time: 79, acton: 6, reward: -71\n",
      "episode: 0/5000, time: 99, acton: 2, reward: -75\n",
      "episode: 0/5000, time: 119, acton: 6, reward: -78\n",
      "episode: 0/5000, time: 139, acton: 6, reward: -105\n",
      "episode: 0/5000, time: 179, acton: 6, reward: -128\n",
      "episode: 0/5000, time: 199, acton: 6, reward: -155\n",
      "episode: 10/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 10/5000, time: 59, acton: 1, reward: -61\n",
      "episode: 10/5000, time: 79, acton: 1, reward: -97\n",
      "episode: 10/5000, time: 139, acton: 1, reward: -148\n",
      "episode: 10/5000, time: 159, acton: 1, reward: -170\n",
      "episode: 20/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 20/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 20/5000, time: 79, acton: 1, reward: -82\n",
      "episode: 20/5000, time: 99, acton: 1, reward: -116\n",
      "episode: 20/5000, time: 119, acton: 1, reward: -149\n",
      "episode: 20/5000, time: 139, acton: 1, reward: -183\n",
      "episode: 20/5000, time: 159, acton: 1, reward: -216\n",
      "episode: 30/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 30/5000, time: 79, acton: 1, reward: -79\n",
      "episode: 30/5000, time: 99, acton: 1, reward: -111\n",
      "episode: 30/5000, time: 119, acton: 1, reward: -144\n",
      "episode: 30/5000, time: 139, acton: 1, reward: -125\n",
      "episode: 30/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 30/5000, time: 199, acton: 1, reward: -222\n",
      "episode: 40/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 40/5000, time: 119, acton: 1, reward: -102\n",
      "episode: 40/5000, time: 179, acton: 1, reward: -182\n",
      "episode: 50/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 50/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 50/5000, time: 79, acton: 1, reward: -86\n",
      "episode: 50/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 50/5000, time: 139, acton: 1, reward: -158\n",
      "episode: 50/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 60/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 60/5000, time: 99, acton: 1, reward: -90\n",
      "episode: 60/5000, time: 139, acton: 1, reward: -138\n",
      "episode: 60/5000, time: 159, acton: 1, reward: -147\n",
      "episode: 60/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 60/5000, time: 199, acton: 1, reward: -221\n",
      "episode: 70/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 70/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 70/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 70/5000, time: 119, acton: 1, reward: -125\n",
      "episode: 70/5000, time: 139, acton: 1, reward: -159\n",
      "episode: 70/5000, time: 159, acton: 1, reward: -193\n",
      "episode: 70/5000, time: 179, acton: 1, reward: -229\n",
      "episode: 80/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 80/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 80/5000, time: 59, acton: 1, reward: -67\n",
      "episode: 80/5000, time: 79, acton: 1, reward: -78\n",
      "episode: 80/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 80/5000, time: 179, acton: 1, reward: -193\n",
      "episode: 90/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 90/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 90/5000, time: 59, acton: 1, reward: -57\n",
      "episode: 90/5000, time: 79, acton: 1, reward: -92\n",
      "episode: 90/5000, time: 99, acton: 1, reward: -95\n",
      "episode: 90/5000, time: 159, acton: 1, reward: -162\n",
      "episode: 90/5000, time: 199, acton: 1, reward: -220\n",
      "episode: 100/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 100/5000, time: 59, acton: 1, reward: -41\n",
      "episode: 100/5000, time: 79, acton: 1, reward: -72\n",
      "episode: 100/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 100/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 100/5000, time: 179, acton: 1, reward: -200\n",
      "episode: 100/5000, time: 199, acton: 1, reward: -212\n",
      "episode: 110/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 110/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 110/5000, time: 59, acton: 1, reward: -44\n",
      "episode: 110/5000, time: 99, acton: 1, reward: -84\n",
      "episode: 110/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 110/5000, time: 139, acton: 1, reward: -135\n",
      "episode: 110/5000, time: 159, acton: 1, reward: -168\n",
      "episode: 110/5000, time: 179, acton: 1, reward: -182\n",
      "episode: 110/5000, time: 199, acton: 1, reward: -196\n",
      "episode: 120/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 120/5000, time: 59, acton: 1, reward: -59\n",
      "episode: 120/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 120/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 120/5000, time: 199, acton: 1, reward: -213\n",
      "episode: 130/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 130/5000, time: 79, acton: 1, reward: -83\n",
      "episode: 130/5000, time: 99, acton: 1, reward: -90\n",
      "episode: 130/5000, time: 119, acton: 1, reward: -129\n",
      "episode: 130/5000, time: 179, acton: 1, reward: -195\n",
      "episode: 140/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 140/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 140/5000, time: 99, acton: 4, reward: -74\n",
      "episode: 140/5000, time: 119, acton: 1, reward: -88\n",
      "episode: 140/5000, time: 139, acton: 1, reward: -125\n",
      "episode: 140/5000, time: 159, acton: 1, reward: -142\n",
      "episode: 140/5000, time: 179, acton: 1, reward: -178\n",
      "episode: 140/5000, time: 199, acton: 1, reward: -209\n",
      "episode: 150/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 150/5000, time: 59, acton: 1, reward: -47\n",
      "episode: 150/5000, time: 79, acton: 1, reward: -67\n",
      "episode: 150/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 150/5000, time: 119, acton: 1, reward: -133\n",
      "episode: 150/5000, time: 139, acton: 1, reward: -167\n",
      "episode: 150/5000, time: 159, acton: 1, reward: -165\n",
      "episode: 150/5000, time: 179, acton: 1, reward: -204\n",
      "episode: 150/5000, time: 199, acton: 1, reward: -237\n",
      "episode: 160/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 160/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 160/5000, time: 59, acton: 1, reward: -66\n",
      "episode: 160/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 160/5000, time: 139, acton: 1, reward: -159\n",
      "episode: 160/5000, time: 159, acton: 1, reward: -193\n",
      "episode: 160/5000, time: 179, acton: 1, reward: -225\n",
      "episode: 160/5000, time: 199, acton: 1, reward: -259\n",
      "episode: 170/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 170/5000, time: 79, acton: 1, reward: -65\n",
      "episode: 170/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 170/5000, time: 119, acton: 1, reward: -113\n",
      "episode: 170/5000, time: 139, acton: 1, reward: -147\n",
      "episode: 170/5000, time: 199, acton: 1, reward: -210\n",
      "episode: 180/5000, time: 19, acton: 2, reward: 0\n",
      "episode: 180/5000, time: 59, acton: 1, reward: -47\n",
      "episode: 180/5000, time: 119, acton: 1, reward: -120\n",
      "episode: 180/5000, time: 159, acton: 1, reward: -172\n",
      "episode: 180/5000, time: 179, acton: 1, reward: -207\n",
      "episode: 190/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 190/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 190/5000, time: 99, acton: 1, reward: -94\n",
      "episode: 190/5000, time: 119, acton: 1, reward: -119\n",
      "episode: 190/5000, time: 139, acton: 1, reward: -151\n",
      "episode: 190/5000, time: 159, acton: 1, reward: -171\n",
      "episode: 190/5000, time: 199, acton: 1, reward: -221\n",
      "episode: 200/5000, time: 59, acton: 1, reward: -46\n",
      "episode: 200/5000, time: 79, acton: 1, reward: -80\n",
      "episode: 200/5000, time: 99, acton: 5, reward: -97\n",
      "episode: 200/5000, time: 119, acton: 1, reward: -125\n",
      "episode: 200/5000, time: 159, acton: 1, reward: -171\n",
      "episode: 200/5000, time: 179, acton: 1, reward: -194\n",
      "episode: 200/5000, time: 199, acton: 1, reward: -198\n",
      "episode: 210/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 210/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 210/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 210/5000, time: 79, acton: 1, reward: -101\n",
      "episode: 210/5000, time: 119, acton: 1, reward: -108\n",
      "episode: 210/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 210/5000, time: 159, acton: 1, reward: -181\n",
      "episode: 210/5000, time: 179, acton: 1, reward: -213\n",
      "episode: 210/5000, time: 199, acton: 1, reward: -218\n",
      "episode: 220/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 220/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 220/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 220/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 220/5000, time: 139, acton: 1, reward: -159\n",
      "episode: 220/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 230/5000, time: 79, acton: 1, reward: -76\n",
      "episode: 230/5000, time: 99, acton: 1, reward: -89\n",
      "episode: 230/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 230/5000, time: 139, acton: 1, reward: -158\n",
      "episode: 230/5000, time: 159, acton: 1, reward: -191\n",
      "episode: 230/5000, time: 179, acton: 1, reward: -226\n",
      "episode: 240/5000, time: 19, acton: 1, reward: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 240/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 240/5000, time: 59, acton: 1, reward: -67\n",
      "episode: 240/5000, time: 79, acton: 1, reward: -68\n",
      "episode: 240/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 240/5000, time: 119, acton: 1, reward: -105\n",
      "episode: 240/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 240/5000, time: 199, acton: 1, reward: -209\n",
      "episode: 250/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 250/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 250/5000, time: 59, acton: 1, reward: -50\n",
      "episode: 250/5000, time: 99, acton: 1, reward: -97\n",
      "episode: 250/5000, time: 119, acton: 1, reward: -130\n",
      "episode: 250/5000, time: 139, acton: 1, reward: -140\n",
      "episode: 250/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 250/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 250/5000, time: 199, acton: 1, reward: -239\n",
      "episode: 260/5000, time: 19, acton: 6, reward: 0\n",
      "episode: 260/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 260/5000, time: 59, acton: 1, reward: -63\n",
      "episode: 260/5000, time: 79, acton: 1, reward: -60\n",
      "episode: 260/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 260/5000, time: 119, acton: 1, reward: -134\n",
      "episode: 260/5000, time: 139, acton: 1, reward: -167\n",
      "episode: 260/5000, time: 199, acton: 1, reward: -208\n",
      "episode: 270/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 270/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 270/5000, time: 99, acton: 1, reward: -80\n",
      "episode: 270/5000, time: 139, acton: 1, reward: -135\n",
      "episode: 270/5000, time: 199, acton: 1, reward: -191\n",
      "episode: 280/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 280/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 280/5000, time: 59, acton: 3, reward: -52\n",
      "episode: 280/5000, time: 79, acton: 1, reward: -80\n",
      "episode: 280/5000, time: 99, acton: 1, reward: -114\n",
      "episode: 280/5000, time: 159, acton: 1, reward: -160\n",
      "episode: 280/5000, time: 179, acton: 1, reward: -192\n",
      "episode: 290/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 290/5000, time: 39, acton: 1, reward: -18\n",
      "episode: 290/5000, time: 79, acton: 1, reward: -73\n",
      "episode: 290/5000, time: 119, acton: 1, reward: -120\n",
      "episode: 290/5000, time: 159, acton: 1, reward: -172\n",
      "episode: 290/5000, time: 199, acton: 1, reward: -226\n",
      "episode: 300/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 300/5000, time: 39, acton: 3, reward: -24\n",
      "episode: 300/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 300/5000, time: 79, acton: 1, reward: -89\n",
      "episode: 300/5000, time: 99, acton: 1, reward: -88\n",
      "episode: 300/5000, time: 119, acton: 1, reward: -130\n",
      "episode: 300/5000, time: 139, acton: 1, reward: -162\n",
      "episode: 300/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 300/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 310/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 310/5000, time: 159, acton: 1, reward: -156\n",
      "episode: 310/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 310/5000, time: 199, acton: 1, reward: -195\n",
      "episode: 320/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 320/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 320/5000, time: 99, acton: 1, reward: -102\n",
      "episode: 320/5000, time: 119, acton: 1, reward: -117\n",
      "episode: 320/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 320/5000, time: 179, acton: 1, reward: -193\n",
      "episode: 320/5000, time: 199, acton: 1, reward: -233\n",
      "episode: 330/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 330/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 330/5000, time: 79, acton: 1, reward: -78\n",
      "episode: 330/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 330/5000, time: 139, acton: 1, reward: -150\n",
      "episode: 330/5000, time: 159, acton: 1, reward: -183\n",
      "episode: 330/5000, time: 179, acton: 1, reward: -216\n",
      "episode: 330/5000, time: 199, acton: 1, reward: -251\n",
      "episode: 340/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 340/5000, time: 79, acton: 1, reward: -61\n",
      "episode: 340/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 340/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 340/5000, time: 199, acton: 1, reward: -201\n",
      "episode: 350/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 350/5000, time: 39, acton: 1, reward: -18\n",
      "episode: 350/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 350/5000, time: 79, acton: 1, reward: -70\n",
      "episode: 350/5000, time: 99, acton: 1, reward: -104\n",
      "episode: 350/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 350/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 350/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 350/5000, time: 199, acton: 1, reward: -214\n",
      "episode: 360/5000, time: 59, acton: 1, reward: -50\n",
      "episode: 360/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 360/5000, time: 119, acton: 1, reward: -123\n",
      "episode: 360/5000, time: 139, acton: 1, reward: -133\n",
      "episode: 360/5000, time: 179, acton: 1, reward: -185\n",
      "episode: 360/5000, time: 199, acton: 1, reward: -224\n",
      "episode: 370/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 370/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 370/5000, time: 59, acton: 1, reward: -64\n",
      "episode: 370/5000, time: 79, acton: 1, reward: -101\n",
      "episode: 370/5000, time: 99, acton: 1, reward: -109\n",
      "episode: 370/5000, time: 159, acton: 1, reward: -176\n",
      "episode: 370/5000, time: 179, acton: 1, reward: -210\n",
      "episode: 370/5000, time: 199, acton: 6, reward: -222\n",
      "episode: 380/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 380/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 380/5000, time: 179, acton: 1, reward: -198\n",
      "episode: 380/5000, time: 199, acton: 1, reward: -233\n",
      "episode: 390/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 390/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 390/5000, time: 59, acton: 1, reward: -47\n",
      "episode: 390/5000, time: 79, acton: 1, reward: -73\n",
      "episode: 390/5000, time: 99, acton: 4, reward: -93\n",
      "episode: 390/5000, time: 119, acton: 1, reward: -105\n",
      "episode: 390/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 390/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 400/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 400/5000, time: 59, acton: 1, reward: -64\n",
      "episode: 400/5000, time: 79, acton: 1, reward: -99\n",
      "episode: 400/5000, time: 99, acton: 1, reward: -105\n",
      "episode: 400/5000, time: 119, acton: 1, reward: -114\n",
      "episode: 400/5000, time: 139, acton: 5, reward: -146\n",
      "episode: 400/5000, time: 179, acton: 1, reward: -191\n",
      "episode: 400/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 410/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 410/5000, time: 79, acton: 1, reward: -84\n",
      "episode: 410/5000, time: 99, acton: 1, reward: -119\n",
      "episode: 410/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 420/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 420/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 420/5000, time: 59, acton: 1, reward: -56\n",
      "episode: 420/5000, time: 79, acton: 1, reward: -94\n",
      "episode: 420/5000, time: 99, acton: 1, reward: -126\n",
      "episode: 420/5000, time: 119, acton: 1, reward: -159\n",
      "episode: 420/5000, time: 159, acton: 1, reward: -179\n",
      "episode: 420/5000, time: 179, acton: 1, reward: -190\n",
      "episode: 420/5000, time: 199, acton: 1, reward: -224\n",
      "episode: 430/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 430/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 430/5000, time: 139, acton: 1, reward: -137\n",
      "episode: 430/5000, time: 159, acton: 1, reward: -144\n",
      "episode: 440/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 440/5000, time: 79, acton: 1, reward: -71\n",
      "episode: 440/5000, time: 99, acton: 1, reward: -105\n",
      "episode: 440/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 440/5000, time: 139, acton: 1, reward: -148\n",
      "episode: 440/5000, time: 159, acton: 1, reward: -182\n",
      "episode: 440/5000, time: 199, acton: 1, reward: -226\n",
      "episode: 450/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 450/5000, time: 79, acton: 1, reward: -74\n",
      "episode: 450/5000, time: 99, acton: 1, reward: -82\n",
      "episode: 450/5000, time: 119, acton: 4, reward: -108\n",
      "episode: 450/5000, time: 179, acton: 1, reward: -175\n",
      "episode: 450/5000, time: 199, acton: 1, reward: -201\n",
      "episode: 460/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 460/5000, time: 59, acton: 1, reward: -55\n",
      "episode: 460/5000, time: 79, acton: 1, reward: -89\n",
      "episode: 460/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 460/5000, time: 199, acton: 1, reward: -224\n",
      "episode: 470/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 470/5000, time: 79, acton: 1, reward: -64\n",
      "episode: 470/5000, time: 119, acton: 1, reward: -107\n",
      "episode: 470/5000, time: 139, acton: 1, reward: -138\n",
      "episode: 470/5000, time: 159, acton: 1, reward: -171\n",
      "episode: 470/5000, time: 179, acton: 1, reward: -164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 480/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 480/5000, time: 59, acton: 1, reward: -48\n",
      "episode: 480/5000, time: 79, acton: 1, reward: -83\n",
      "episode: 480/5000, time: 119, acton: 1, reward: -111\n",
      "episode: 480/5000, time: 139, acton: 1, reward: -152\n",
      "episode: 480/5000, time: 159, acton: 1, reward: -184\n",
      "episode: 480/5000, time: 179, acton: 1, reward: -219\n",
      "episode: 480/5000, time: 199, acton: 1, reward: -254\n",
      "episode: 490/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 490/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 490/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 490/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 490/5000, time: 119, acton: 1, reward: -138\n",
      "episode: 490/5000, time: 159, acton: 1, reward: -178\n",
      "episode: 490/5000, time: 179, acton: 1, reward: -210\n",
      "episode: 490/5000, time: 199, acton: 1, reward: -244\n",
      "episode: 500/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 500/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 500/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 500/5000, time: 119, acton: 1, reward: -122\n",
      "episode: 500/5000, time: 139, acton: 1, reward: -155\n",
      "episode: 500/5000, time: 159, acton: 1, reward: -188\n",
      "episode: 500/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 510/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 510/5000, time: 79, acton: 1, reward: -77\n",
      "episode: 510/5000, time: 99, acton: 1, reward: -111\n",
      "episode: 510/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 510/5000, time: 139, acton: 1, reward: -156\n",
      "episode: 510/5000, time: 159, acton: 1, reward: -190\n",
      "episode: 510/5000, time: 179, acton: 1, reward: -197\n",
      "episode: 520/5000, time: 19, acton: 5, reward: 0\n",
      "episode: 520/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 520/5000, time: 79, acton: 1, reward: -75\n",
      "episode: 520/5000, time: 99, acton: 1, reward: -109\n",
      "episode: 520/5000, time: 119, acton: 1, reward: -111\n",
      "episode: 520/5000, time: 139, acton: 1, reward: -150\n",
      "episode: 520/5000, time: 159, acton: 1, reward: -184\n",
      "episode: 520/5000, time: 179, acton: 1, reward: -219\n",
      "episode: 530/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 530/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 530/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 530/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 530/5000, time: 119, acton: 1, reward: -134\n",
      "episode: 530/5000, time: 139, acton: 1, reward: -167\n",
      "episode: 530/5000, time: 159, acton: 1, reward: -199\n",
      "episode: 530/5000, time: 199, acton: 1, reward: -229\n",
      "episode: 540/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 540/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 540/5000, time: 99, acton: 1, reward: -100\n",
      "episode: 540/5000, time: 119, acton: 1, reward: -132\n",
      "episode: 540/5000, time: 139, acton: 1, reward: -147\n",
      "episode: 540/5000, time: 159, acton: 1, reward: -180\n",
      "episode: 540/5000, time: 179, acton: 6, reward: -193\n",
      "episode: 540/5000, time: 199, acton: 1, reward: -214\n",
      "episode: 550/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 550/5000, time: 59, acton: 3, reward: -42\n",
      "episode: 550/5000, time: 79, acton: 1, reward: -73\n",
      "episode: 550/5000, time: 99, acton: 1, reward: -99\n",
      "episode: 550/5000, time: 119, acton: 1, reward: -132\n",
      "episode: 550/5000, time: 179, acton: 1, reward: -204\n",
      "episode: 560/5000, time: 59, acton: 6, reward: -41\n",
      "episode: 560/5000, time: 99, acton: 1, reward: -96\n",
      "episode: 560/5000, time: 199, acton: 1, reward: -208\n",
      "episode: 570/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 570/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 570/5000, time: 79, acton: 1, reward: -82\n",
      "episode: 570/5000, time: 99, acton: 1, reward: -116\n",
      "episode: 570/5000, time: 119, acton: 1, reward: -148\n",
      "episode: 570/5000, time: 139, acton: 1, reward: -140\n",
      "episode: 570/5000, time: 159, acton: 1, reward: -184\n",
      "episode: 570/5000, time: 199, acton: 1, reward: -225\n",
      "episode: 580/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 580/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 580/5000, time: 59, acton: 1, reward: -59\n",
      "episode: 580/5000, time: 79, acton: 1, reward: -94\n",
      "episode: 580/5000, time: 99, acton: 1, reward: -92\n",
      "episode: 580/5000, time: 179, acton: 1, reward: -180\n",
      "episode: 580/5000, time: 199, acton: 1, reward: -205\n",
      "episode: 590/5000, time: 79, acton: 1, reward: -70\n",
      "episode: 590/5000, time: 119, acton: 1, reward: -106\n",
      "episode: 590/5000, time: 159, acton: 1, reward: -161\n",
      "episode: 590/5000, time: 199, acton: 1, reward: -213\n",
      "episode: 600/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 600/5000, time: 59, acton: 1, reward: -64\n",
      "episode: 600/5000, time: 99, acton: 1, reward: -107\n",
      "episode: 600/5000, time: 119, acton: 1, reward: -141\n",
      "episode: 600/5000, time: 139, acton: 1, reward: -175\n",
      "episode: 600/5000, time: 159, acton: 1, reward: -152\n",
      "episode: 600/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 610/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 610/5000, time: 59, acton: 1, reward: -45\n",
      "episode: 610/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 610/5000, time: 159, acton: 1, reward: -172\n",
      "episode: 610/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 620/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 620/5000, time: 39, acton: 1, reward: -26\n",
      "episode: 620/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 620/5000, time: 119, acton: 1, reward: -117\n",
      "episode: 620/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 620/5000, time: 199, acton: 1, reward: -222\n",
      "episode: 630/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 630/5000, time: 79, acton: 1, reward: -75\n",
      "episode: 630/5000, time: 99, acton: 1, reward: -107\n",
      "episode: 630/5000, time: 119, acton: 1, reward: -142\n",
      "episode: 630/5000, time: 139, acton: 1, reward: -176\n",
      "episode: 630/5000, time: 159, acton: 1, reward: -156\n",
      "episode: 630/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 630/5000, time: 199, acton: 1, reward: -239\n",
      "episode: 640/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 640/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 640/5000, time: 139, acton: 1, reward: -133\n",
      "episode: 640/5000, time: 199, acton: 1, reward: -212\n",
      "episode: 650/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 650/5000, time: 59, acton: 1, reward: -48\n",
      "episode: 650/5000, time: 99, acton: 1, reward: -98\n",
      "episode: 650/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 650/5000, time: 139, acton: 1, reward: -147\n",
      "episode: 650/5000, time: 159, acton: 1, reward: -177\n",
      "episode: 650/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 650/5000, time: 199, acton: 1, reward: -197\n",
      "episode: 660/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 660/5000, time: 59, acton: 1, reward: -45\n",
      "episode: 660/5000, time: 79, acton: 1, reward: -80\n",
      "episode: 660/5000, time: 99, acton: 1, reward: -113\n",
      "episode: 660/5000, time: 159, acton: 1, reward: -174\n",
      "episode: 670/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 670/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 670/5000, time: 59, acton: 1, reward: -57\n",
      "episode: 670/5000, time: 79, acton: 1, reward: -92\n",
      "episode: 670/5000, time: 119, acton: 1, reward: -114\n",
      "episode: 670/5000, time: 139, acton: 1, reward: -153\n",
      "episode: 670/5000, time: 179, acton: 1, reward: -196\n",
      "episode: 670/5000, time: 199, acton: 1, reward: -206\n",
      "episode: 680/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 680/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 680/5000, time: 59, acton: 1, reward: -49\n",
      "episode: 680/5000, time: 139, acton: 1, reward: -120\n",
      "episode: 680/5000, time: 159, acton: 1, reward: -143\n",
      "episode: 680/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 680/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 690/5000, time: 79, acton: 1, reward: -71\n",
      "episode: 690/5000, time: 99, acton: 1, reward: -91\n",
      "episode: 690/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 690/5000, time: 179, acton: 1, reward: -200\n",
      "episode: 690/5000, time: 199, acton: 1, reward: -210\n",
      "episode: 700/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 700/5000, time: 99, acton: 1, reward: -101\n",
      "episode: 700/5000, time: 119, acton: 1, reward: -133\n",
      "episode: 700/5000, time: 139, acton: 7, reward: -150\n",
      "episode: 700/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 700/5000, time: 199, acton: 1, reward: -240\n",
      "episode: 710/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 710/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 710/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 710/5000, time: 119, acton: 1, reward: -133\n",
      "episode: 710/5000, time: 139, acton: 1, reward: -166\n",
      "episode: 710/5000, time: 179, acton: 1, reward: -209\n",
      "episode: 720/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 720/5000, time: 39, acton: 1, reward: -24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 720/5000, time: 59, acton: 1, reward: -46\n",
      "episode: 720/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 720/5000, time: 99, acton: 1, reward: -99\n",
      "episode: 720/5000, time: 119, acton: 1, reward: -131\n",
      "episode: 720/5000, time: 179, acton: 1, reward: -177\n",
      "episode: 720/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 730/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 730/5000, time: 59, acton: 1, reward: -51\n",
      "episode: 730/5000, time: 79, acton: 1, reward: -76\n",
      "episode: 730/5000, time: 99, acton: 1, reward: -110\n",
      "episode: 730/5000, time: 119, acton: 1, reward: -126\n",
      "episode: 730/5000, time: 139, acton: 1, reward: -158\n",
      "episode: 730/5000, time: 159, acton: 1, reward: -192\n",
      "episode: 730/5000, time: 179, acton: 1, reward: -225\n",
      "episode: 730/5000, time: 199, acton: 1, reward: -227\n",
      "episode: 740/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 740/5000, time: 59, acton: 5, reward: -48\n",
      "episode: 740/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 740/5000, time: 139, acton: 1, reward: -154\n",
      "episode: 740/5000, time: 159, acton: 1, reward: -186\n",
      "episode: 750/5000, time: 39, acton: 1, reward: -18\n",
      "episode: 750/5000, time: 59, acton: 1, reward: -50\n",
      "episode: 750/5000, time: 79, acton: 1, reward: -85\n",
      "episode: 750/5000, time: 99, acton: 1, reward: -119\n",
      "episode: 750/5000, time: 119, acton: 1, reward: -152\n",
      "episode: 750/5000, time: 139, acton: 1, reward: -185\n",
      "episode: 750/5000, time: 159, acton: 1, reward: -173\n",
      "episode: 750/5000, time: 199, acton: 1, reward: -205\n",
      "episode: 760/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 760/5000, time: 59, acton: 1, reward: -46\n",
      "episode: 760/5000, time: 79, acton: 1, reward: -81\n",
      "episode: 760/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 760/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 770/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 770/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 770/5000, time: 99, acton: 1, reward: -92\n",
      "episode: 770/5000, time: 139, acton: 1, reward: -139\n",
      "episode: 770/5000, time: 159, acton: 1, reward: -174\n",
      "episode: 770/5000, time: 179, acton: 1, reward: -207\n",
      "episode: 780/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 780/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 780/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 780/5000, time: 79, acton: 1, reward: -101\n",
      "episode: 780/5000, time: 99, acton: 1, reward: -136\n",
      "episode: 780/5000, time: 139, acton: 1, reward: -149\n",
      "episode: 780/5000, time: 159, acton: 1, reward: -188\n",
      "episode: 780/5000, time: 179, acton: 1, reward: -194\n",
      "episode: 780/5000, time: 199, acton: 1, reward: -234\n",
      "episode: 790/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 790/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 790/5000, time: 79, acton: 1, reward: -72\n",
      "episode: 790/5000, time: 99, acton: 6, reward: -91\n",
      "episode: 790/5000, time: 119, acton: 1, reward: -116\n",
      "episode: 790/5000, time: 159, acton: 1, reward: -165\n",
      "episode: 790/5000, time: 179, acton: 7, reward: -188\n",
      "episode: 790/5000, time: 199, acton: 1, reward: -200\n",
      "episode: 800/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 800/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 800/5000, time: 59, acton: 1, reward: -65\n",
      "episode: 800/5000, time: 79, acton: 1, reward: -77\n",
      "episode: 800/5000, time: 99, acton: 1, reward: -103\n",
      "episode: 800/5000, time: 119, acton: 1, reward: -136\n",
      "episode: 800/5000, time: 199, acton: 1, reward: -226\n",
      "episode: 810/5000, time: 19, acton: 5, reward: 0\n",
      "episode: 810/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 810/5000, time: 119, acton: 6, reward: -118\n",
      "episode: 810/5000, time: 159, acton: 1, reward: -164\n",
      "episode: 810/5000, time: 199, acton: 1, reward: -206\n",
      "episode: 820/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 820/5000, time: 59, acton: 1, reward: -52\n",
      "episode: 820/5000, time: 79, acton: 1, reward: -87\n",
      "episode: 820/5000, time: 99, acton: 1, reward: -118\n",
      "episode: 820/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 820/5000, time: 179, acton: 1, reward: -205\n",
      "episode: 820/5000, time: 199, acton: 1, reward: -242\n",
      "episode: 830/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 830/5000, time: 59, acton: 1, reward: -62\n",
      "episode: 830/5000, time: 79, acton: 1, reward: -70\n",
      "episode: 830/5000, time: 99, acton: 1, reward: -89\n",
      "episode: 830/5000, time: 159, acton: 1, reward: -159\n",
      "episode: 830/5000, time: 179, acton: 1, reward: -183\n",
      "episode: 830/5000, time: 199, acton: 1, reward: -209\n",
      "episode: 840/5000, time: 39, acton: 1, reward: -28\n",
      "episode: 840/5000, time: 59, acton: 1, reward: -42\n",
      "episode: 840/5000, time: 79, acton: 1, reward: -69\n",
      "episode: 840/5000, time: 99, acton: 1, reward: -104\n",
      "episode: 840/5000, time: 139, acton: 1, reward: -148\n",
      "episode: 840/5000, time: 159, acton: 1, reward: -182\n",
      "episode: 840/5000, time: 179, acton: 1, reward: -190\n",
      "episode: 840/5000, time: 199, acton: 1, reward: -217\n",
      "episode: 850/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 850/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 850/5000, time: 59, acton: 1, reward: -56\n",
      "episode: 850/5000, time: 79, acton: 1, reward: -91\n",
      "episode: 850/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 850/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 850/5000, time: 199, acton: 1, reward: -220\n",
      "episode: 860/5000, time: 39, acton: 1, reward: -30\n",
      "episode: 860/5000, time: 59, acton: 1, reward: -44\n",
      "episode: 860/5000, time: 79, acton: 1, reward: -79\n",
      "episode: 860/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 860/5000, time: 139, acton: 1, reward: -155\n",
      "episode: 860/5000, time: 179, acton: 1, reward: -199\n",
      "episode: 870/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 870/5000, time: 39, acton: 1, reward: -32\n",
      "episode: 870/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 870/5000, time: 99, acton: 1, reward: -106\n",
      "episode: 870/5000, time: 139, acton: 1, reward: -154\n",
      "episode: 870/5000, time: 159, acton: 1, reward: -187\n",
      "episode: 870/5000, time: 179, acton: 1, reward: -189\n",
      "episode: 870/5000, time: 199, acton: 1, reward: -219\n",
      "episode: 880/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 880/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 880/5000, time: 59, acton: 1, reward: -36\n",
      "episode: 880/5000, time: 79, acton: 1, reward: -74\n",
      "episode: 880/5000, time: 99, acton: 6, reward: -89\n",
      "episode: 880/5000, time: 119, acton: 1, reward: -118\n",
      "episode: 880/5000, time: 139, acton: 1, reward: -138\n",
      "episode: 880/5000, time: 159, acton: 1, reward: -170\n",
      "episode: 880/5000, time: 179, acton: 1, reward: -204\n",
      "episode: 890/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 890/5000, time: 39, acton: 6, reward: -16\n",
      "episode: 890/5000, time: 59, acton: 1, reward: -43\n",
      "episode: 890/5000, time: 79, acton: 1, reward: -75\n",
      "episode: 890/5000, time: 99, acton: 1, reward: -95\n",
      "episode: 890/5000, time: 119, acton: 1, reward: -128\n",
      "episode: 890/5000, time: 139, acton: 1, reward: -160\n",
      "episode: 890/5000, time: 179, acton: 1, reward: -198\n",
      "episode: 890/5000, time: 199, acton: 1, reward: -231\n",
      "episode: 900/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 900/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 900/5000, time: 79, acton: 1, reward: -88\n",
      "episode: 900/5000, time: 119, acton: 1, reward: -120\n",
      "episode: 900/5000, time: 139, acton: 1, reward: -152\n",
      "episode: 900/5000, time: 159, acton: 1, reward: -186\n",
      "episode: 900/5000, time: 179, acton: 1, reward: -184\n",
      "episode: 900/5000, time: 199, acton: 1, reward: -222\n",
      "episode: 910/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 910/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 910/5000, time: 59, acton: 1, reward: -53\n",
      "episode: 910/5000, time: 79, acton: 1, reward: -89\n",
      "episode: 910/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 910/5000, time: 159, acton: 1, reward: -161\n",
      "episode: 910/5000, time: 199, acton: 1, reward: -216\n",
      "episode: 920/5000, time: 19, acton: 7, reward: 0\n",
      "episode: 920/5000, time: 39, acton: 1, reward: -20\n",
      "episode: 920/5000, time: 119, acton: 1, reward: -113\n",
      "episode: 920/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 930/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 930/5000, time: 39, acton: 1, reward: -24\n",
      "episode: 930/5000, time: 59, acton: 1, reward: -54\n",
      "episode: 930/5000, time: 119, acton: 1, reward: -121\n",
      "episode: 930/5000, time: 159, acton: 1, reward: -169\n",
      "episode: 930/5000, time: 179, acton: 1, reward: -202\n",
      "episode: 930/5000, time: 199, acton: 1, reward: -237\n",
      "episode: 940/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 940/5000, time: 39, acton: 1, reward: -22\n",
      "episode: 940/5000, time: 99, acton: 1, reward: -89\n",
      "episode: 940/5000, time: 119, acton: 1, reward: -124\n",
      "episode: 940/5000, time: 139, acton: 7, reward: -139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 940/5000, time: 179, acton: 5, reward: -190\n",
      "episode: 940/5000, time: 199, acton: 1, reward: -193\n",
      "episode: 950/5000, time: 19, acton: 1, reward: 0\n",
      "episode: 950/5000, time: 39, acton: 1, reward: -21\n",
      "episode: 950/5000, time: 59, acton: 1, reward: -43\n",
      "episode: 950/5000, time: 99, acton: 5, reward: -88\n",
      "episode: 950/5000, time: 119, acton: 1, reward: -111\n",
      "episode: 950/5000, time: 159, acton: 1, reward: -167\n",
      "episode: 950/5000, time: 179, acton: 1, reward: -172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-b9d0f636450d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 所有车辆的平均行驶时间，除以总时间后越大则越好，[0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#         average_travel_time = eng.get_average_travel_time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-48d4bd3998f4>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_expected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_critic_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "for i in range(args.max_episode):\n",
    "    env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    s = env.get_state()\n",
    "    s = np.array(list(s['start_lane_vehicle_count'].values()) + \n",
    "                 [s['current_phase']])\n",
    "    s = np.reshape(s, [1, s_dim])\n",
    "    s = s.astype(np.float32)\n",
    "#     s = torch.tensor(s)\n",
    "#     print(trainer.choose_action(s))\n",
    "    last_action = phase_list[int(trainer.choose_action(torch.tensor(s)))]\n",
    "    \n",
    "    while t < args.max_step:\n",
    "        a_choice = trainer.choose_action(torch.tensor(s))\n",
    "        a = phase_list[int(a_choice)]\n",
    "            \n",
    "        if a == last_action:\n",
    "            env.step(a)\n",
    "        else:\n",
    "            for _ in range(env.yellow_time):\n",
    "                env.step(0)\n",
    "                t += 1\n",
    "                flag = (t >= args.max_step)\n",
    "                if flag:\n",
    "                    break\n",
    "            if flag:\n",
    "                break\n",
    "            env.step(a)\n",
    "            \n",
    "        last_action = a\n",
    "        t += 1\n",
    "        next_state = env.get_state()\n",
    "        r = env.get_reward()\n",
    "        next_state = np.array(list(next_state['start_lane_vehicle_count'].values()) + \n",
    "                              [next_state['current_phase']])\n",
    "        next_state = np.reshape(next_state, [1, s_dim])\n",
    "        next_state = next_state.astype(np.float32)\n",
    "#         next_state = torch.tensor(next_state)\n",
    "        \n",
    "        trainer.replay_buffer.add(s, a_choice, r, next_state)\n",
    "        s = next_state\n",
    "        \n",
    "        total_time = t + i * args.max_step\n",
    "        if total_time > args.learning_start and total_time % args.update_freq == 0:\n",
    "            trainer.optimize()\n",
    "        # 所有车辆的平均行驶时间，除以总时间后越大则越好，[0,1]\n",
    "#         average_travel_time = eng.get_average_travel_time()\n",
    "#         reward_travel_time = eng.get_current_time()/average_travel_time\n",
    "        \n",
    "        if i % 10 == 0 and t % 20 == 0:\n",
    "            print(\"episode: {}/{}, time: {}, acton: {}, reward: {}\"\n",
    "              .format(i, args.max_episode, t-1, a, r))\n",
    "\n",
    "#     if i%100 == 0:\n",
    "#         trainer.save_models(i)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
